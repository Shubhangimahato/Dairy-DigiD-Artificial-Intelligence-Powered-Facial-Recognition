{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM+C98vixG7Si/yMTCeAurx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhangimahato/Dairy-DigiD-Artificial-Intelligence-Powered-Facial-Recognition/blob/main/Dairy_DigiD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L86zKylPxIk9"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show pyyaml\n"
      ],
      "metadata": {
        "id": "uFRKbUWExUjg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "FbTnDaFtxWuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi\n",
        "!nvcc --version #10.1\n",
        "!python --version # 3.7.6"
      ],
      "metadata": {
        "id": "vdLIV3GsxZLF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "qDnkdVaKxZVM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())\n"
      ],
      "metadata": {
        "id": "EcBK-YlXxZX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Detectron 2"
      ],
      "metadata": {
        "id": "sSt0I7MuxibI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install \"git+https://github.com/facebookresearch/detectron2.git\"\n"
      ],
      "metadata": {
        "id": "2FEHEobrxZao"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.structures import BoxMode\n"
      ],
      "metadata": {
        "id": "MjdjLZdXxZdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "OHLL2KJOxtc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "# Define the root directories for both '28 May' and '24 June'\n",
        "root_dirs = {\n",
        "    '28 May': '/content/drive/My Drive/28 May_00101/28 May/',\n",
        "    '24 June': '/content/drive/My Drive/24 June/'\n",
        "}\n",
        "\n",
        "# Define the subdirectories to include for both '28 May' and '24 June'\n",
        "subdirs_to_include = {\n",
        "    '28 May': ['28 May_001', '28 May_002'],\n",
        "    '24 June': ['24_june_001', '24_june_002', '24_june_003', '24_june_004']\n",
        "}\n",
        "\n",
        "# Initialize lists to collect all images and annotations\n",
        "all_annotations = []\n",
        "\n",
        "# Loop through the root directories and their specified subdirectories\n",
        "for root_dir_name, subdirs in subdirs_to_include.items():\n",
        "    root_dir = root_dirs[root_dir_name]\n",
        "    for subdir_name in subdirs:\n",
        "        subdir_path = os.path.join(root_dir, subdir_name)\n",
        "        annotation_folder = os.path.join(subdir_path, 'annotations')\n",
        "        images_root_folder = os.path.join(subdir_path, 'images')\n",
        "\n",
        "        # Load all JSON annotation files from the annotation folder\n",
        "        for annotation_file in os.listdir(annotation_folder):\n",
        "            if annotation_file.endswith('.json'):\n",
        "                json_path = os.path.join(annotation_folder, annotation_file)\n",
        "                with open(json_path, 'r') as f:\n",
        "                    annotations = json.load(f)\n",
        "                    for annotation in annotations['annotations']:\n",
        "                        annotation['image_id'] = f\"{subdir_name}_{annotation['image_id']}\"\n",
        "                    for image_info in annotations['images']:\n",
        "                        image_info['id'] = f\"{subdir_name}_{image_info['id']}\"\n",
        "\n",
        "                        # Remove redundant folder name from 'file_name' if present\n",
        "                        if image_info['file_name'].startswith(f\"{root_dir_name}/\"):\n",
        "                            image_info['file_name'] = image_info['file_name'][len(root_dir_name) + 1:]\n",
        "\n",
        "                        # Now combine with 'images_root_folder'\n",
        "                        image_info['file_name'] = os.path.join(images_root_folder, image_info['file_name'])\n",
        "\n",
        "                    all_annotations.append({\n",
        "                        'images': annotations['images'],\n",
        "                        'annotations': annotations['annotations']\n",
        "                    })\n",
        "\n",
        "# Now create the dataset dictionary\n",
        "def custom_cattle_dataset():\n",
        "    dataset_dicts = []\n",
        "    for annotation_group in all_annotations:\n",
        "        images_info_map = {img_info['id']: img_info for img_info in annotation_group['images']}\n",
        "        for annotation in annotation_group['annotations']:\n",
        "            record = {}\n",
        "            image_info = images_info_map.get(annotation['image_id'])\n",
        "            if image_info is None:\n",
        "                print(f\"Image info not found for image ID: {annotation['image_id']}\")\n",
        "                continue\n",
        "            record['file_name'] = image_info['file_name']\n",
        "            record['image_id'] = image_info['id']\n",
        "            record['height'] = image_info['height']\n",
        "            record['width'] = image_info['width']\n",
        "            # Determine category based on file path\n",
        "            if \"2 Years old or less\" in record[\"file_name\"] or \"Heifer\" in record[\"file_name\"]:\n",
        "                category_id = 0\n",
        "            elif \"Dry Cows\" in record[\"file_name\"] or \"dry cows\" in record[\"file_name\"]:\n",
        "                category_id = 1\n",
        "            elif \"Mature Milking Cow\" in record[\"file_name\"]:\n",
        "                category_id = 2\n",
        "            elif \"Pregnant\" in record[\"file_name\"]:\n",
        "                category_id = 3\n",
        "            else:\n",
        "                category_id = 999  # Undefined category (optional)\n",
        "            obj = {\n",
        "                'bbox': annotation['bbox'],\n",
        "                'bbox_mode': BoxMode.XYWH_ABS,\n",
        "                'category_id': category_id,\n",
        "                'segmentation': annotation.get('segmentation', []),\n",
        "                'keypoints': annotation.get('keypoints', []),\n",
        "                'iscrowd': annotation.get('iscrowd', 0),\n",
        "            }\n",
        "            record['annotations'] = [obj]\n",
        "            dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "# Generate the dataset dictionary\n",
        "dataset_dicts = custom_cattle_dataset()\n",
        "print(\"Dataset Dicts Sample:\", dataset_dicts[:5])\n"
      ],
      "metadata": {
        "id": "mCknFSozxZgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_images_by_category(dataset_dicts):\n",
        "    category_count = {\n",
        "        \"Pregnant\": 0,\n",
        "        \"Mature Milking Cow\": 0,\n",
        "        \"Dry Cows\": 0,\n",
        "        \"2 Years old or less\": 0\n",
        "    }\n",
        "    category_mapping = {\n",
        "        0: \"2 Years old or less\",\n",
        "        1: \"Dry Cows\",\n",
        "        2: \"Mature Milking Cow\",\n",
        "        3: \"Pregnant\",\n",
        "        999: \"Undefined\"\n",
        "    }\n",
        "    for record in dataset_dicts:\n",
        "        if record['annotations']:\n",
        "            category_id = record['annotations'][0]['category_id']\n",
        "            category_label = category_mapping.get(category_id, \"Undefined\")\n",
        "            if category_label in category_count:\n",
        "                category_count[category_label] += 1\n",
        "    return category_count\n",
        "\n",
        "category_image_count = count_images_by_category(dataset_dicts)\n",
        "print(category_image_count)\n"
      ],
      "metadata": {
        "id": "wcwX9dzGxZi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the keypoint flip map using numerical points\n",
        "keypoint_flip_map = [\n",
        "    (1, 5),   # Left Eye extreme right <-> Right Eye extreme left\n",
        "    (2, 6),   # Left Eye extreme left <-> Right Eye extreme righta\n",
        "    (3, 7),   # Left Eye extreme top <-> Right Eye extreme top\n",
        "    (4, 8),   # Left Eye extreme bottom <-> Right Eye extreme bottom\n",
        "    (9, 14),  # Left Ear extreme top right <-> Right Ear extreme top left\n",
        "    (10, 15), # Left Ear extreme left <-> Right Ear extreme right\n",
        "    (11, 16), # Left Ear extreme top mid <-> Right Ear extreme top mid\n",
        "    (12, 17), # Left Ear extreme bottom mid <-> Right Ear extreme bottom mid\n",
        "    (13, 18), # Left Ear extreme bottom right <-> Right Ear extreme bottom left\n",
        "    (19, 21), # Muzzle Top left <-> Muzzle Top right\n",
        "    (20, 20), # Muzzle Top mid <-> Muzzle Top mid (no change)\n",
        "    (21, 19), # Muzzle Top right <-> Muzzle Top left\n",
        "    (22, 24), # Muzzle Bottom right <-> Muzzle Bottom left\n",
        "    (23, 23), # Muzzle Bottom mid <-> Muzzle Bottom mid (no change)\n",
        "    (24, 22), # Muzzle Bottom left <-> Muzzle Bottom right\n",
        "    (28, 30), # Head left side <-> Head right side\n",
        "    (29, 29), # Head extreme top <-> Head extreme top (no change)\n",
        "    (30, 28), # Head right side <-> Head left side\n",
        "]\n",
        "\n",
        "# Define the keypoint names for 30 points\n",
        "keypoint_names = [\n",
        "    'kp1', 'kp2', 'kp3', 'kp4', 'kp5', 'kp6', 'kp7', 'kp8', 'kp9', 'kp10',\n",
        "    'kp11', 'kp12', 'kp13', 'kp14', 'kp15', 'kp16', 'kp17', 'kp18', 'kp19', 'kp20',\n",
        "    'kp21', 'kp22', 'kp23', 'kp24', 'kp25', 'kp26', 'kp27', 'kp28', 'kp29', 'kp30'\n",
        "]"
      ],
      "metadata": {
        "id": "yKKxFB5AxZlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "# Function to split dataset into train, val, and test sets\n",
        "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    assert train_ratio + val_ratio + test_ratio == 1, \"Ratios must sum up to 1!\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    shuffled_dataset = random.sample(dataset, len(dataset))\n",
        "\n",
        "    # Compute split indices\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(total_size * train_ratio)\n",
        "    val_size = int(total_size * val_ratio)\n",
        "\n",
        "    # Split dataset\n",
        "    train_set = shuffled_dataset[:train_size]\n",
        "    val_set = shuffled_dataset[train_size:train_size + val_size]\n",
        "    test_set = shuffled_dataset[train_size + val_size:]\n",
        "\n",
        "    return train_set, val_set, test_set\n",
        "\n",
        "# Ensure your dataset is already loaded in `dataset_dicts`\n",
        "train_set, val_set, test_set = split_dataset(dataset_dicts)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Train Set Size: {len(train_set)}\")\n",
        "print(f\"Validation Set Size: {len(val_set)}\")\n",
        "print(f\"Test Set Size: {len(test_set)}\")\n",
        "\n",
        "# Define functions to return datasets\n",
        "def get_cows_train():\n",
        "    return train_set\n",
        "\n",
        "def get_cows_val():\n",
        "    return val_set\n",
        "\n",
        "def get_cows_test():\n",
        "    return test_set\n",
        "\n",
        "# Register datasets\n",
        "DatasetCatalog.register(\"cows_train\", get_cows_train)\n",
        "DatasetCatalog.register(\"cows_val\", get_cows_val)\n",
        "DatasetCatalog.register(\"cows_test\", get_cows_test)\n",
        "\n",
        "# Define common metadata\n",
        "metadata_info = {\n",
        "    \"keypoint_flip_map\": keypoint_flip_map,\n",
        "    \"keypoint_names\": keypoint_names,\n",
        "    \"thing_classes\": [\"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"],\n",
        "    \"evaluator_type\": \"coco\"\n",
        "}\n",
        "\n",
        "# Apply metadata to all datasets\n",
        "for dataset_name in [\"cows_train\", \"cows_val\", \"cows_test\"]:\n",
        "    MetadataCatalog.get(dataset_name).keypoint_flip_map = metadata_info[\"keypoint_flip_map\"]\n",
        "    MetadataCatalog.get(dataset_name).keypoint_names = metadata_info[\"keypoint_names\"]\n",
        "    MetadataCatalog.get(dataset_name).thing_classes = metadata_info[\"thing_classes\"]\n",
        "    MetadataCatalog.get(dataset_name).evaluator_type = metadata_info[\"evaluator_type\"]\n",
        "\n",
        "print(\"Datasets registered successfully with metadata!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "Lq7BNirIxZnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# K Fold Validation"
      ],
      "metadata": {
        "id": "z-bm0rfzzjm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "# Define the model creation function\n",
        "def create_model(learning_rate, num_filters, kernel_size):\n",
        "    model = Sequential([\n",
        "        Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(4, activation='softmax')  # Adjust based on the number of classes\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss=SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold = 1\n",
        "fold_accuracies = []\n",
        "fold_f1_scores = []\n",
        "\n",
        "# Convert data to numpy arrays for KFold\n",
        "X = train_images_processed\n",
        "y = train_labels_processed\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "    print(f\"Training Fold {fold}/{k_folds}...\")\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_model(learning_rate=0.001, num_filters=32, kernel_size=3)\n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=0,\n",
        "                        validation_data=(X_val, y_val))\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_val_pred_classes)\n",
        "    f1 = f1_score(y_val, y_val_pred_classes, average='weighted')\n",
        "    fold_accuracies.append(accuracy)\n",
        "    fold_f1_scores.append(f1)\n",
        "\n",
        "    print(f\"Fold {fold} - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}\")\n",
        "    fold += 1\n",
        "\n",
        "# Statistical Analysis\n",
        "mean_accuracy = np.mean(fold_accuracies)\n",
        "std_accuracy = np.std(fold_accuracies)\n",
        "mean_f1 = np.mean(fold_f1_scores)\n",
        "std_f1 = np.std(fold_f1_scores)\n",
        "\n",
        "print(\"\\nCross-Validation Results:\")\n",
        "print(f\"Mean Accuracy: {mean_accuracy:.4f}, Std Accuracy: {std_accuracy:.4f}\")\n",
        "print(f\"Mean F1-Score: {mean_f1:.4f}, Std F1-Score: {std_f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "fPqrPe5szp10"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, precision_score, recall_score, roc_curve\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the model creation function\n",
        "def create_model(learning_rate, num_filters, kernel_size):\n",
        "    model = Sequential([\n",
        "        Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(4, activation='softmax')  # Adjust based on number of classes\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss=SparseCategoricalCrossentropy(from_logits=False),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# K-Fold Cross-Validation\n",
        "k_folds = 5\n",
        "kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "\n",
        "fold = 1\n",
        "fold_accuracies = []\n",
        "fold_f1_scores = []\n",
        "fold_auc_scores = []\n",
        "\n",
        "# Convert data to numpy arrays for KFold\n",
        "X = train_images_processed\n",
        "y = train_labels_processed\n",
        "\n",
        "for train_index, val_index in kf.split(X):\n",
        "    print(f\"Training Fold {fold}/{k_folds}...\")\n",
        "    X_train, X_val = X[train_index], X[val_index]\n",
        "    y_train, y_val = y[train_index], y[val_index]\n",
        "\n",
        "    # Create and train the model\n",
        "    model = create_model(learning_rate=0.001, num_filters=32, kernel_size=3)\n",
        "    history = model.fit(X_train, y_train, epochs=10, batch_size=64, verbose=0,\n",
        "                        validation_data=(X_val, y_val))\n",
        "\n",
        "    # Evaluate the model on the validation set\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    y_val_pred_classes = np.argmax(y_val_pred, axis=1)\n",
        "\n",
        "    accuracy = accuracy_score(y_val, y_val_pred_classes)\n",
        "    f1 = f1_score(y_val, y_val_pred_classes, average='weighted')\n",
        "    auc = roc_auc_score(y_val, y_val_pred, multi_class='ovr', average='weighted')\n",
        "\n",
        "    fold_accuracies.append(accuracy)\n",
        "    fold_f1_scores.append(f1)\n",
        "    fold_auc_scores.append(auc)\n",
        "\n",
        "    print(f\"Fold {fold} - Accuracy: {accuracy:.4f}, F1-Score: {f1:.4f}, AUC-ROC: {auc:.4f}\")\n",
        "    fold += 1\n",
        "\n",
        "# Statistical Analysis (Validation Performance)\n",
        "mean_accuracy = np.mean(fold_accuracies)\n",
        "std_accuracy = np.std(fold_accuracies)\n",
        "mean_f1 = np.mean(fold_f1_scores)\n",
        "std_f1 = np.std(fold_f1_scores)\n",
        "mean_auc = np.mean(fold_auc_scores)\n",
        "std_auc = np.std(fold_auc_scores)\n",
        "\n",
        "print(\"\\nValidation Cross-Validation Results:\")\n",
        "print(f\"Mean Accuracy: {mean_accuracy:.4f}, Std Accuracy: {std_accuracy:.4f}\")\n",
        "print(f\"Mean F1-Score: {mean_f1:.4f}, Std F1-Score: {std_f1:.4f}\")\n",
        "print(f\"Mean AUC-ROC: {mean_auc:.4f}, Std AUC-ROC: {std_auc:.4f}\")\n",
        "\n",
        "# ----------------- Final Model Training and Test Evaluation -----------------\n",
        "\n",
        "print(\"\\nTraining Final Model on Full Training Data...\")\n",
        "final_model = create_model(learning_rate=0.001, num_filters=32, kernel_size=3)\n",
        "final_model.fit(X, y, epochs=10, batch_size=64, verbose=1)\n",
        "\n",
        "# Evaluate on test data\n",
        "print(\"\\nEvaluating on Test Set...\")\n",
        "y_test_pred = final_model.predict(test_images_processed)\n",
        "y_test_pred_classes = np.argmax(y_test_pred, axis=1)\n",
        "\n",
        "test_accuracy = accuracy_score(test_labels_processed, y_test_pred_classes)\n",
        "test_f1 = f1_score(test_labels_processed, y_test_pred_classes, average='weighted')\n",
        "test_auc = roc_auc_score(test_labels_processed, y_test_pred, multi_class='ovr', average='weighted')\n",
        "\n",
        "print(\"\\nTest Set Results:\")\n",
        "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "print(f\"Test F1-Score: {test_f1:.4f}\")\n",
        "print(f\"Test AUC-ROC: {test_auc:.4f}\")\n",
        "\n",
        "# Plot ROC Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "for i in range(y_test_pred.shape[1]):\n",
        "    fpr, tpr, _ = roc_curve(test_labels_processed == i, y_test_pred[:, i])\n",
        "    plt.plot(fpr, tpr, label=f'Class {i}')\n",
        "\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve for Test Set')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nKWLmKd2zp4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "_CjhVgV8zT9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
        "\n",
        "def create_model(learning_rate, num_filters, kernel_size):\n",
        "    model = Sequential([\n",
        "        Conv2D(num_filters, (kernel_size, kernel_size), activation='relu', input_shape=(28, 28, 1)),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Conv2D(num_filters * 2, (kernel_size, kernel_size), activation='relu'),\n",
        "        MaxPooling2D((2, 2)),\n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(24, activation='softmax')\n",
        "    ])\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate),\n",
        "                  loss=SparseCategoricalCrossentropy(from_logits=True),\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "learning_rates = [0.001, 0.0001, 0.01]\n",
        "num_filters = [32, 64, 128]\n",
        "kernel_sizes = [3, 5]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "for lr in learning_rates:\n",
        "    for nf in num_filters:\n",
        "        for ks in kernel_sizes:\n",
        "            model = create_model(lr, nf, ks)\n",
        "            history = model.fit(train_images_processed, train_labels_processed, epochs=5, batch_size=64,\n",
        "                                validation_data=(val_images_processed, val_labels_processed), verbose=0)\n",
        "            val_accuracy = history.history['val_accuracy'][-1]\n",
        "            if val_accuracy > best_accuracy:\n",
        "                best_accuracy = val_accuracy\n",
        "                best_params = {'learning_rate': lr, 'num_filters': nf, 'kernel_size': ks}\n",
        "\n",
        "print(\"Best Hyperparameters from Random Search:\", best_params)\n",
        "print(\"Best Validation Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "id": "ZmeMWO0MzbkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.001  # Fixed as it performs well\n",
        "num_filters_range = [64, 96, 128]\n",
        "kernel_size_range = [3, 4, 5]\n",
        "\n",
        "best_accuracy = 0\n",
        "best_params = {}\n",
        "\n",
        "for nf in num_filters_range:\n",
        "    for ks in kernel_size_range:\n",
        "        model = create_model(learning_rate, nf, ks)\n",
        "        history = model.fit(train_images_processed, train_labels_processed, epochs=5, batch_size=64,\n",
        "                            validation_data=(val_images_processed, val_labels_processed), verbose=0)\n",
        "        val_accuracy = history.history['val_accuracy'][-1]\n",
        "        if val_accuracy > best_accuracy:\n",
        "            best_accuracy = val_accuracy\n",
        "            best_params = {'num_filters': nf, 'kernel_size': ks}\n",
        "\n",
        "print(\"Best Hyperparameters after Refined Grid Search:\", best_params)\n",
        "print(\"Best Validation Accuracy:\", best_accuracy)\n"
      ],
      "metadata": {
        "id": "dodR-fAGzbmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "D3qDhrYbySVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.modeling import build_model\n",
        "from detectron2.modeling.meta_arch import GeneralizedRCNN\n",
        "from detectron2.modeling.roi_heads import StandardROIHeads\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "class FocalLossRCNN(GeneralizedRCNN):\n",
        "    def forward(self, batched_inputs):\n",
        "        if self.training:\n",
        "            losses = super().forward(batched_inputs)\n",
        "            # Modify losses[\"loss_cls\"] with a custom focal loss\n",
        "            losses[\"loss_cls\"] = focal_loss_function(self.pred_class_logits, self.gt_classes)\n",
        "            return losses\n",
        "        else:\n",
        "            return super().forward(batched_inputs)\n",
        "\n",
        "# Integrate into Trainer\n",
        "cfg.MODEL.ROI_HEADS.LOSS = \"FocalLossRCNN\"\n"
      ],
      "metadata": {
        "id": "vx4Swtx1xZqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.BOX_LOSS_TYPE = \"siou\"  # Custom loss integration\n",
        "cfg.SOLVER.OPTIMIZER = \"AdamW\"\n",
        "cfg.SOLVER.BASE_LR = 0.0001  # Adjust learning rate for AdamW\n"
      ],
      "metadata": {
        "id": "LdhxUgFXxZsr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data.samplers import RepeatFactorTrainingSampler\n",
        "\n",
        "def build_train_loader(cfg):\n",
        "    dataset_dicts = DatasetCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "    repeat_factors = RepeatFactorTrainingSampler.repeat_factors_from_category_frequency(\n",
        "        dataset_dicts, repeat_thresh=0.1\n",
        "    )\n",
        "    return build_detection_train_loader(cfg, sampler=RepeatFactorTrainingSampler(repeat_factors))\n"
      ],
      "metadata": {
        "id": "m8GF8eAIxZuy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.MODEL.BACKBONE.NAME = \"build_resnet_fpn_backbone\"\n",
        "cfg.MODEL.RESNETS.DEPTH = 101\n",
        "cfg.MODEL.RESNETS.OUT_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n"
      ],
      "metadata": {
        "id": "uyWKXO1XxZyL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.FPN.NORM = \"GN\"  # Group Normalization for FPN\n",
        "cfg.MODEL.FPN.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\"]  # Include lower and higher layers\n"
      ],
      "metadata": {
        "id": "LDy1eN3lyepu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.BACKBONE.ATTENTION_TYPE = \"CoordinateAttention\"  # Requires custom implementation\n"
      ],
      "metadata": {
        "id": "ibl2b6kmyesZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64, 128, 256, 512]]  # Adjust based on dataset\n",
        "cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0, 2.0]]  # Common ratios\n"
      ],
      "metadata": {
        "id": "ev6HBBMyyeu5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Without Argumentation\n",
        "\n",
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.engine import HookBase\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import numpy as np\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "\n",
        "# AugmentedTrainer and custom_mapper should be defined before this cell.\n",
        "# EarlyStopping class should also be defined if you're using early stopping.\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"cows_train_filtered\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.SOLVER.MAX_ITER = 2000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 64\n",
        "cfg.SOLVER.IMS_PER_BATCH = 8\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
        "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 30\n",
        "cfg.TEST.KEYPOINT_OKS_SIGMAS = np.ones((30, 1), dtype=float).tolist()\n",
        "cfg.SOLVER.STEPS = [3000, 3800]\n",
        "cfg.SOLVER.GAMMA = 0.1\n",
        "\n",
        "# Define the keypoint flip map using numerical points\n",
        "keypoint_flip_map = [\n",
        "    (1, 5), (2, 6), (3, 7), (4, 8), (9, 14), (10, 15), (11, 16), (12, 17),\n",
        "    (13, 18), (19, 21), (20, 20), (21, 19), (22, 24), (23, 23), (24, 22),\n",
        "    (28, 30), (29, 29), (30, 28)\n",
        "]\n",
        "\n",
        "# Define the keypoint names for 30 points\n",
        "keypoint_names = [f'kp{i}' for i in range(1, 31)]\n",
        "\n",
        "# Keypoint metadata setup\n",
        "MetadataCatalog.get(\"cows_train_filtered\").keypoint_flip_map = keypoint_flip_map\n",
        "MetadataCatalog.get(\"cows_train_filtered\").keypoint_names = keypoint_names\n",
        "MetadataCatalog.get(\"cows_train_filtered\").thing_classes = [\n",
        "    \"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"\n",
        "]\n",
        "MetadataCatalog.get(\"cows_train_filtered\").evaluator_type = \"coco\"\n",
        "\n",
        "# Function to filter out missing files\n",
        "def check_alternative_extensions(file_path):\n",
        "    \"\"\"Check if a file exists with a different extension (case mismatch fix).\"\"\"\n",
        "    base, _ = os.path.splitext(file_path)\n",
        "    extensions = [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\", \".png\"]\n",
        "    for ext in extensions:\n",
        "        new_path = base + ext\n",
        "        if os.path.exists(new_path):\n",
        "            return new_path  # Return the corrected path\n",
        "    return None  # No alternative found\n",
        "\n",
        "def filter_missing_files(dataset_dicts):\n",
        "    \"\"\"Filter dataset to exclude missing files or correct paths with case mismatch.\"\"\"\n",
        "    filtered_dataset = []\n",
        "    for record in dataset_dicts:\n",
        "        file_path = record[\"file_name\"]\n",
        "        if os.path.exists(file_path):\n",
        "            filtered_dataset.append(record)\n",
        "        else:\n",
        "            corrected_path = check_alternative_extensions(file_path)\n",
        "            if corrected_path:\n",
        "                record[\"file_name\"] = corrected_path  # Update file path\n",
        "                filtered_dataset.append(record)\n",
        "    return filtered_dataset\n",
        "\n",
        "# Register the filtered dataset\n",
        "dataset_dicts = DatasetCatalog.get(\"cows_train\")\n",
        "filtered_dataset_dicts = filter_missing_files(dataset_dicts)\n",
        "\n",
        "# Dataset registration function\n",
        "def get_filtered_cows_train():\n",
        "    return filtered_dataset_dicts\n",
        "\n",
        "# Re-registering the dataset\n",
        "if \"cows_train_filtered\" in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(\"cows_train_filtered\")\n",
        "    MetadataCatalog.remove(\"cows_train_filtered\")\n",
        "\n",
        "DatasetCatalog.register(\"cows_train_filtered\", get_filtered_cows_train)\n",
        "MetadataCatalog.get(\"cows_train_filtered\").keypoint_flip_map = keypoint_flip_map\n",
        "MetadataCatalog.get(\"cows_train_filtered\").keypoint_names = keypoint_names\n",
        "MetadataCatalog.get(\"cows_train_filtered\").thing_classes = [\n",
        "    \"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"\n",
        "]\n",
        "MetadataCatalog.get(\"cows_train_filtered\").evaluator_type = \"coco\"\n",
        "\n",
        "# Set output directory\n",
        "cfg.OUTPUT_DIR = \"./my_custom_output_dir\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.build_train_loader = build_train_loader\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "GBxQYYjKyexi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# With Argumentation\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import numpy as np\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data import build_detection_train_loader\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# AugmentedTrainer and custom_mapper should be defined before this cell.\n",
        "# EarlyStopping class should also be defined if you're using early stopping.\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"cows_train_filtered\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.SOLVER.MAX_ITER = 2000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 32\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
        "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 30\n",
        "cfg.TEST.KEYPOINT_OKS_SIGMAS = np.ones((30, 1), dtype=float).tolist()\n",
        "cfg.SOLVER.STEPS = [3000, 3800]\n",
        "cfg.SOLVER.GAMMA = 0.1\n",
        "\n",
        "# Define the keypoint flip map using numerical points\n",
        "keypoint_flip_map = [\n",
        "    (1, 5), (2, 6), (3, 7), (4, 8), (9, 14), (10, 15), (11, 16), (12, 17),\n",
        "    (13, 18), (19, 21), (20, 20), (21, 19), (22, 24), (23, 23), (24, 22),\n",
        "    (28, 30), (29, 29), (30, 28)\n",
        "]\n",
        "\n",
        "# Define the keypoint names for 30 points\n",
        "keypoint_names = [f'kp{i}' for i in range(1, 31)]\n",
        "\n",
        "# Keypoint metadata setup\n",
        "MetadataCatalog.get(\"cows_train_filtered\").keypoint_flip_map = keypoint_flip_map\n",
        "MetadataCatalog.get(\"cows_train_filtered\").keypoint_names = keypoint_names\n",
        "MetadataCatalog.get(\"cows_train_filtered\").thing_classes = [\n",
        "    \"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"\n",
        "]\n",
        "MetadataCatalog.get(\"cows_train_filtered\").evaluator_type = \"coco\"\n",
        "\n",
        "# Function to filter out missing files\n",
        "def check_alternative_extensions(file_path):\n",
        "    \"\"\"Check if a file exists with a different extension (case mismatch fix).\"\"\"\n",
        "    base, _ = os.path.splitext(file_path)\n",
        "    extensions = [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\", \".png\"]\n",
        "    for ext in extensions:\n",
        "        new_path = base + ext\n",
        "        if os.path.exists(new_path):\n",
        "            return new_path  # Return the corrected path\n",
        "    return None  # No alternative found\n",
        "\n",
        "def filter_missing_files(dataset_dicts):\n",
        "    \"\"\"Filter dataset to exclude missing files or correct paths with case mismatch.\"\"\"\n",
        "    filtered_dataset = []\n",
        "    for record in dataset_dicts:\n",
        "        file_path = record[\"file_name\"]\n",
        "        if os.path.exists(file_path):\n",
        "            filtered_dataset.append(record)\n",
        "        else:\n",
        "            corrected_path = check_alternative_extensions(file_path)\n",
        "            if corrected_path:\n",
        "                record[\"file_name\"] = corrected_path  # Update file path\n",
        "                filtered_dataset.append(record)\n",
        "    return filtered_dataset\n",
        "\n",
        "# Register the filtered dataset\n",
        "dataset_dicts = DatasetCatalog.get(\"cows_train\")\n",
        "filtered_dataset_dicts = filter_missing_files(dataset_dicts)\n",
        "\n",
        "# Dataset registration function\n",
        "def get_filtered_cows_train():\n",
        "    return filtered_dataset_dicts\n",
        "\n",
        "# Re-registering the dataset\n",
        "if \"cows_train_filtered\" in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(\"cows_train_filtered\")\n",
        "    MetadataCatalog.remove(\"cows_train_filtered\")\n",
        "\n",
        "DatasetCatalog.register(\"cows_train_filtered\", get_filtered_cows_train)\n",
        "MetadataCatalog.get(\"cows_train_filtered\").keypoint_flip_map = keypoint_flip_map\n",
        "MetadataCatalog.get(\"cows_train_filtered\").keypoint_names = keypoint_names\n",
        "MetadataCatalog.get(\"cows_train_filtered\").thing_classes = [\n",
        "    \"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"\n",
        "]\n",
        "MetadataCatalog.get(\"cows_train_filtered\").evaluator_type = \"coco\"\n",
        "\n",
        "# Basic augmentations for all classes\n",
        "def standard_augmentations(image):\n",
        "    transform_list = [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.MotionBlur(p=0.3),\n",
        "        A.Rotate(limit=10, p=0.7),\n",
        "        A.RandomCrop(height=int(image.shape[0] * 0.85), width=int(image.shape[1] * 0.85), p=0.7),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        "    augmented = A.Compose(transform_list)(image=image)\n",
        "    return augmented[\"image\"].permute(1, 2, 0).numpy()\n",
        "\n",
        "# Additional augmentations for \"Pregnant\" class\n",
        "def pregnant_extra_augmentations(image):\n",
        "    transform_list = [\n",
        "        A.GaussNoise(var_limit=50.0, p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.7),\n",
        "        A.MotionBlur(p=0.5),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        "    augmented = A.Compose(transform_list)(image=image)\n",
        "    return augmented[\"image\"].permute(1, 2, 0).numpy()\n",
        "\n",
        "# Custom Mapper\n",
        "def custom_mapper(dataset_dict):\n",
        "    dataset_dict = dataset_dict.copy()\n",
        "    image = cv2.imread(dataset_dict[\"file_name\"])\n",
        "    if image is None:\n",
        "        return None\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "    category_id = dataset_dict[\"annotations\"][0][\"category_id\"]\n",
        "    image = standard_augmentations(image)\n",
        "    if category_id == 3:  # Apply additional augmentations for Pregnant class\n",
        "        image = pregnant_extra_augmentations(image)\n",
        "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1))\n",
        "    return dataset_dict\n",
        "\n",
        "# Define Custom Trainer\n",
        "class CustomTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        return build_detection_train_loader(cfg, mapper=custom_mapper)\n",
        "\n",
        "# Set output directory\n",
        "cfg.OUTPUT_DIR = \"./my_custom_output_dir\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "trainer = CustomTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "IvkKBNpIyez6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify source and new destination paths\n",
        "source_path = \"./my_custom_output_dir/model_final.pth\"\n",
        "destination_path = \"/content/drive/My Drive/model1_final.pth\"  # Change this to the desired path in your Google Drive\n",
        "\n",
        "# Copy the model file using shutil\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(f\"Model copied to {destination_path}\")\n"
      ],
      "metadata": {
        "id": "D1Y5mcUX0chg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.data import build_detection_test_loader, MetadataCatalog, DatasetCatalog\n",
        "import numpy as np\n",
        "\n",
        "# Load the config file\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TEST = (\"cows_val_filtered\",)\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = \"./my_custom_output_dir/model_final.pth\"  # Path to the trained model\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
        "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 30\n",
        "cfg.TEST.KEYPOINT_OKS_SIGMAS = np.ones((30, 1), dtype=float).tolist()\n",
        "cfg.OUTPUT_DIR = \"./my_custom_output_dir\"\n",
        "\n",
        "# Define keypoint metadata\n",
        "keypoint_flip_map = [\n",
        "    (1, 5), (2, 6), (3, 7), (4, 8), (9, 14), (10, 15), (11, 16), (12, 17),\n",
        "    (13, 18), (19, 21), (20, 20), (21, 19), (22, 24), (23, 23), (24, 22),\n",
        "    (28, 30), (29, 29), (30, 28)\n",
        "]\n",
        "keypoint_names = [f'kp{i}' for i in range(1, 31)]\n",
        "\n",
        "# Register validation dataset\n",
        "dataset_dicts = DatasetCatalog.get(\"cows_val\")\n",
        "\n",
        "# Ensure dataset paths are valid\n",
        "def filter_missing_files(dataset_dicts):\n",
        "    filtered_dataset = []\n",
        "    for record in dataset_dicts:\n",
        "        if os.path.exists(record[\"file_name\"]):\n",
        "            filtered_dataset.append(record)\n",
        "    return filtered_dataset\n",
        "\n",
        "filtered_dataset_dicts = filter_missing_files(dataset_dicts)\n",
        "\n",
        "if \"cows_val_filtered\" in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(\"cows_val_filtered\")\n",
        "    MetadataCatalog.remove(\"cows_val_filtered\")\n",
        "\n",
        "DatasetCatalog.register(\"cows_val_filtered\", lambda: filtered_dataset_dicts)\n",
        "MetadataCatalog.get(\"cows_val_filtered\").keypoint_flip_map = keypoint_flip_map\n",
        "MetadataCatalog.get(\"cows_val_filtered\").keypoint_names = keypoint_names\n",
        "MetadataCatalog.get(\"cows_val_filtered\").thing_classes = [\n",
        "    \"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"\n",
        "]\n",
        "MetadataCatalog.get(\"cows_val_filtered\").evaluator_type = \"coco\"\n",
        "\n",
        "# Run evaluation\n",
        "evaluator = COCOEvaluator(\"cows_val_filtered\", cfg, False, output_dir=cfg.OUTPUT_DIR)\n",
        "val_loader = build_detection_test_loader(cfg, \"cows_val_filtered\")\n",
        "inference_on_dataset(DefaultTrainer.build_model(cfg), val_loader, evaluator)"
      ],
      "metadata": {
        "id": "t4JFLqOc3WzM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results"
      ],
      "metadata": {
        "id": "sPQRz76P0nMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AUC-ROC** Curve"
      ],
      "metadata": {
        "id": "TraLv_0F0THZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "categories = [\"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"]\n",
        "\n",
        "# Load model weights and set up predictor\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.DATASETS.TEST = (\"cows_train_filtered\",)\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "actual_labels = []\n",
        "predicted_probs = []\n",
        "\n",
        "def extract_ground_truth_from_filepath(file_path):\n",
        "    if \"Young Cows\" in file_path or \"2 Years old or less\" in file_path or \"Heifer\" in file_path:\n",
        "        return 0  # Category index\n",
        "    elif \"Dry Cows\" in file_path:\n",
        "        return 1\n",
        "    elif \"Mature Milking Cow\" in file_path:\n",
        "        return 2\n",
        "    elif \"Pregnant\" in file_path:\n",
        "        return 3\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Process images for evaluation\n",
        "for root, dirs, files in os.walk('/content/drive/My Drive/test'):\n",
        "    for file in files:\n",
        "        if file.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "            file_path = os.path.join(root, file)\n",
        "            image = cv2.imread(file_path)\n",
        "            if image is None:\n",
        "                print(f\"Error loading image: {file_path}\")\n",
        "                continue\n",
        "\n",
        "            # Run inference\n",
        "            outputs = predictor(image)\n",
        "            probs = outputs[\"instances\"].scores.cpu().numpy() if len(outputs[\"instances\"]) > 0 else []\n",
        "            classes = outputs[\"instances\"].pred_classes.cpu().numpy() if len(outputs[\"instances\"]) > 0 else []\n",
        "\n",
        "            ground_truth = extract_ground_truth_from_filepath(file_path)\n",
        "            if ground_truth is not None:\n",
        "                actual_labels.append(ground_truth)\n",
        "\n",
        "                # Assign predicted probabilities\n",
        "                if len(probs) > 0:\n",
        "                    prob_vector = np.zeros(len(categories))  # Initialize probabilities for all classes\n",
        "                    for cls, prob in zip(classes, probs):\n",
        "                        prob_vector[cls] = prob\n",
        "                    prob_vector = F.softmax(torch.tensor(prob_vector), dim=0).numpy()  # Normalize with softmax\n",
        "                    predicted_probs.append(prob_vector)\n",
        "                else:\n",
        "                    predicted_probs.append(np.zeros(len(categories)))  # No prediction\n",
        "\n",
        "# Convert to numpy arrays\n",
        "actual_labels = np.array(actual_labels)\n",
        "predicted_probs = np.array(predicted_probs)\n",
        "\n",
        "# Calculate AUC-ROC\n",
        "try:\n",
        "    auc_roc = roc_auc_score(actual_labels, predicted_probs, multi_class=\"ovr\", average=\"weighted\")\n",
        "    print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
        "except ValueError as e:\n",
        "    print(f\"Error calculating AUC-ROC: {e}\")\n"
      ],
      "metadata": {
        "id": "br8ZR1twye2f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Calculate ROC curve and AUC for each class\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "roc_auc = {}\n",
        "\n",
        "for i in range(len(categories)):\n",
        "    # Binarize the actual labels for the current class\n",
        "    binary_actual = (actual_labels == i).astype(int)\n",
        "    fpr[i], tpr[i], _ = roc_curve(binary_actual, predicted_probs[:, i])\n",
        "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "\n",
        "# Plot all ROC curves\n",
        "plt.figure(figsize=(10, 7))\n",
        "for i in range(len(categories)):\n",
        "    plt.plot(\n",
        "        fpr[i], tpr[i],\n",
        "        label=f\"Class {categories[i]} (AUC = {roc_auc[i]:.2f})\"\n",
        "    )\n",
        "\n",
        "# Add random chance line\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", linestyle=\"--\")\n",
        "\n",
        "# Graph styling\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Multi-Class ROC Curve\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nraFRxl1ye4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "categories = [\"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"]\n",
        "\n",
        "# Load model weights and set up predictor\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/My Drive/model1_final.pth\"\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n",
        "cfg.DATASETS.TEST = (\"cows_test\",)\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "predicted_labels = []\n",
        "actual_labels = []\n",
        "\n",
        "def extract_ground_truth_from_filepath(file_path):\n",
        "    # Returns category label based on file path\n",
        "    if \"Young Cows\" in file_path or \"2 Years old or less\" in file_path or \"Heifer\" in file_path:\n",
        "        return \"Young Cows\"\n",
        "    elif \"Dry Cows\" in file_path:\n",
        "        return \"Dry Cows\"\n",
        "    elif \"Mature Milking Cow\" in file_path:\n",
        "        return \"Mature Milking Cow\"\n",
        "    elif \"Pregnant\" in file_path:\n",
        "        return \"Pregnant\"\n",
        "    else:\n",
        "        return None  # Return None for undefined categories\n",
        "\n",
        "def process_images_for_evaluation(folder_path, output_folder):\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "    for root, dirs, files in os.walk(folder_path):\n",
        "        for file in files:\n",
        "            if file.endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                file_path = os.path.join(root, file)\n",
        "                image = cv2.imread(file_path)\n",
        "                if image is None:\n",
        "                    print(f\"Error loading image: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Run inference\n",
        "                outputs = predictor(image)\n",
        "                metadata = MetadataCatalog.get(cfg.DATASETS.TEST[0])\n",
        "                visualizer = Visualizer(image[:, :, ::-1], metadata=metadata, scale=1.2)\n",
        "                out = visualizer.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
        "\n",
        "                # Save visualized output\n",
        "                output_image = out.get_image()[:, :, ::-1]\n",
        "                relative_path = os.path.relpath(root, folder_path)\n",
        "                output_subfolder = os.path.join(output_folder, relative_path)\n",
        "                os.makedirs(output_subfolder, exist_ok=True)\n",
        "                output_image_path = os.path.join(output_subfolder, file)\n",
        "                cv2.imwrite(output_image_path, output_image)\n",
        "\n",
        "                # Skip if no predictions are made\n",
        "                if len(outputs[\"instances\"].pred_classes) == 0:\n",
        "                    print(f\"No prediction for image: {file_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Map predictions and ground truth labels\n",
        "                predicted_class_idx = outputs[\"instances\"].pred_classes[0].item()\n",
        "                predicted_class = categories[predicted_class_idx]\n",
        "                actual_class = extract_ground_truth_from_filepath(file_path)\n",
        "\n",
        "                # Append only valid actual labels\n",
        "                if actual_class is not None:\n",
        "                    predicted_labels.append(predicted_class)\n",
        "                    actual_labels.append(actual_class)\n",
        "\n",
        "# Run evaluation\n",
        "input_folder = '/content/drive/My Drive/test'\n",
        "output_folder = '/content/drive/My Drive/test_results'\n",
        "process_images_for_evaluation(input_folder, output_folder)\n",
        "\n",
        "# Print evaluation metrics\n",
        "print(\"Confusion Matrix:\")\n",
        "print(confusion_matrix(actual_labels, predicted_labels, labels=categories))\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(actual_labels, predicted_labels, target_names=categories))\n"
      ],
      "metadata": {
        "id": "Krz0TmO6ye7M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ensemble_predictions(image, predictors):\n",
        "    results = []\n",
        "    for predictor in predictors:\n",
        "        outputs = predictor(image)\n",
        "        results.append(outputs[\"instances\"].pred_classes.tolist())\n",
        "    # Aggregate predictions using majority voting\n",
        "    aggregated = Counter([item for sublist in results for item in sublist])\n",
        "    return aggregated.most_common(1)[0][0]  # Return the most common class\n"
      ],
      "metadata": {
        "id": "b0TqKnhnye9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate classification report\n",
        "report = classification_report(actual_labels, predicted_labels, target_names=categories, output_dict=True)\n",
        "\n",
        "# Extract metrics for visualization\n",
        "precision = [report[cls]['precision'] for cls in categories]\n",
        "recall = [report[cls]['recall'] for cls in categories]\n",
        "f1_score = [report[cls]['f1-score'] for cls in categories]\n",
        "\n",
        "# Radar Chart\n",
        "def plot_radar_chart(categories, precision, recall, f1_score):\n",
        "    metrics = ['Precision', 'Recall', 'F1-Score']\n",
        "    values = [precision, recall, f1_score]\n",
        "    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False).tolist()\n",
        "    angles += angles[:1]\n",
        "\n",
        "    values = [list(metric) + [metric[0]] for metric in values]\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))\n",
        "    for idx, value in enumerate(values):\n",
        "        ax.plot(angles, value, label=metrics[idx], linewidth=2)\n",
        "        ax.fill(angles, value, alpha=0.25)\n",
        "\n",
        "    ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "    ax.set_xticks(angles[:-1])\n",
        "    ax.set_xticklabels(categories)\n",
        "    ax.set_title(\"Classification Metrics Radar Chart\")\n",
        "    ax.legend(loc=\"upper right\", bbox_to_anchor=(1.2, 1.2))\n",
        "    plt.show()\n",
        "\n",
        "plot_radar_chart(categories, precision, recall, f1_score)\n",
        "\n",
        "# Bar Chart for Comparison\n",
        "def plot_bar_chart(categories, precision, recall, f1_score):\n",
        "    x = np.arange(len(categories))\n",
        "    width = 0.25\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 5))\n",
        "    ax.bar(x - width, precision, width, label=\"Precision\", color=\"skyblue\")\n",
        "    ax.bar(x, recall, width, label=\"Recall\", color=\"lightgreen\")\n",
        "    ax.bar(x + width, f1_score, width, label=\"F1-Score\", color=\"salmon\")\n",
        "\n",
        "    ax.set_xlabel(\"Categories\")\n",
        "    ax.set_ylabel(\"Scores\")\n",
        "    ax.set_title(\"Classification Metrics Comparison\")\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(categories)\n",
        "    ax.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_bar_chart(categories, precision, recall, f1_score)"
      ],
      "metadata": {
        "id": "6hUG3yBJyfAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "# Generate the confusion matrix\n",
        "conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=categories)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "# Generate the classification report\n",
        "class_report = classification_report(actual_labels, predicted_labels, target_names=categories, output_dict=True)\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(actual_labels, predicted_labels, target_names=categories))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "def plot_confusion_matrix(conf_matrix, categories):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=categories, yticklabels=categories)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.ylabel('True')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.show()\n",
        "\n",
        "# Plot Precision, Recall, F1-Score\n",
        "def plot_classification_report(class_report, categories):\n",
        "    metrics = ['precision', 'recall', 'f1-score']\n",
        "    scores = {metric: [class_report[category][metric] for category in categories] for metric in metrics}\n",
        "\n",
        "    x = np.arange(len(categories))  # label locations\n",
        "    width = 0.2  # width of the bars\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "    rects1 = ax.bar(x - width, scores['precision'], width, label='Precision')\n",
        "    rects2 = ax.bar(x, scores['recall'], width, label='Recall')\n",
        "    rects3 = ax.bar(x + width, scores['f1-score'], width, label='F1-Score')\n",
        "\n",
        "    ax.set_xlabel('Categories')\n",
        "    ax.set_ylabel('Scores')\n",
        "    ax.set_title('Classification Metrics by Category')\n",
        "    ax.set_xticks(x)\n",
        "    ax.set_xticklabels(categories)\n",
        "    ax.legend()\n",
        "\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrix\n",
        "plot_confusion_matrix(conf_matrix, categories)\n",
        "\n",
        "# Plot classification report metrics\n",
        "plot_classification_report(class_report, categories)\n"
      ],
      "metadata": {
        "id": "KnBlqWSeyfCn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Set the path to the metrics.json file\n",
        "output_dir = cfg.OUTPUT_DIR\n",
        "metrics_file = os.path.join(output_dir, \"metrics.json\")\n",
        "\n",
        "# Load the metrics from the JSON file\n",
        "metrics = []\n",
        "with open(metrics_file, 'r') as f:\n",
        "    for line in f:\n",
        "        metrics.append(json.loads(line))\n",
        "\n",
        "# Extract the iteration values, training loss, and validation loss\n",
        "iterations = [x['iteration'] for x in metrics if 'total_loss' in x]\n",
        "train_losses = [x['total_loss'] for x in metrics if 'total_loss' in x]\n",
        "\n",
        "# Validation loss may have different key, make sure to check the key in your JSON\n",
        "val_iterations = [x['iteration'] for x in metrics if 'validation_loss' in x]\n",
        "val_losses = [x['validation_loss'] for x in metrics if 'validation_loss' in x]\n",
        "\n",
        "# Plot both the training and validation loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(iterations, train_losses, label='Training Loss', color='blue')\n",
        "if len(val_iterations) > 0 and len(val_losses) > 0:\n",
        "    plt.plot(val_iterations, val_losses, label='Validation Loss', color='red')\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss over Iterations')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Eh8787zGyfFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir = '/content/drive/MyDrive/detectron2_models'\n",
        "os.makedirs(output_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "RRpu0E8UyfHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "# Path where your model is saved after training\n",
        "final_model_path = \"./my_custom_output_dir/model_final.pth\"\n",
        "\n",
        "# Target path in Google Drive\n",
        "saved_model_path = \"/content/drive/My Drive/model/model_final.pth\"\n",
        "\n",
        "# Ensure the target directory exists\n",
        "target_dir = \"/content/drive/My Drive/model\"\n",
        "os.makedirs(target_dir, exist_ok=True)\n",
        "\n",
        "# Copy the model file to Google Drive\n",
        "shutil.copy(final_model_path, saved_model_path)\n",
        "\n",
        "print(f\"Model saved to {saved_model_path}\")"
      ],
      "metadata": {
        "id": "USgIahA2yfKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRAD Cam"
      ],
      "metadata": {
        "id": "RGZbibDh1deg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam\n"
      ],
      "metadata": {
        "id": "ZFSnAJR1yfNh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2 import model_zoo\n",
        "\n",
        "# Load the trained model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Adjust based on your classes\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/My Drive/model1_final.pth\"  # Change to your model path\n",
        "model = build_model(cfg)\n",
        "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
        "model.eval()\n",
        "\n",
        "# Grad-CAM Hook Functions\n",
        "feature_maps = None\n",
        "gradients = None\n",
        "\n",
        "def forward_hook(module, input, output):\n",
        "    global feature_maps\n",
        "    feature_maps = output\n",
        "\n",
        "def backward_hook(module, grad_in, grad_out):\n",
        "    global gradients\n",
        "    gradients = grad_out[0]\n",
        "\n",
        "# Register hooks to a deeper convolutional layer (res4 for better feature visualization)\n",
        "target_layer = model.backbone.bottom_up.res4\n",
        "target_layer.register_forward_hook(forward_hook)\n",
        "target_layer.register_full_backward_hook(backward_hook)\n",
        "\n",
        "# Function to generate Grad-CAM\n",
        "def generate_grad_cam(image_path):\n",
        "    global feature_maps, gradients\n",
        "\n",
        "    torch.cuda.empty_cache()  # Free up GPU memory\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Unable to load image at '{image_path}'. Check the path.\")\n",
        "        return\n",
        "\n",
        "    img = cv2.resize(img, (512, 512))  # Reduce image size to avoid memory overflow\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Convert to tensor and send to GPU\n",
        "    img_tensor = torch.tensor(img).permute(2, 0, 1).float().unsqueeze(0).cuda()\n",
        "\n",
        "    # Forward pass\n",
        "    model.zero_grad()\n",
        "    inputs = [{\"image\": img_tensor.squeeze()}]\n",
        "    outputs = model(inputs)[0]  # Extract first output dictionary\n",
        "\n",
        "    if \"instances\" not in outputs:\n",
        "        print(\"Error: No instances detected in the output.\")\n",
        "        return\n",
        "\n",
        "    instances = outputs[\"instances\"]\n",
        "    if len(instances) == 0:\n",
        "        print(\"No objects detected in this image.\")\n",
        "        return\n",
        "\n",
        "    # Compute gradients for the highest scoring class\n",
        "    scores = instances.scores\n",
        "    highest_class_idx = scores.argmax()\n",
        "    scores[highest_class_idx].backward()\n",
        "\n",
        "    # **Fix tensor mismatch issue**\n",
        "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])  # Average over height and width\n",
        "    feature_maps = feature_maps.squeeze(0)  # Remove batch dimension\n",
        "\n",
        "    # Reshape pooled gradients to match feature maps\n",
        "    pooled_gradients = pooled_gradients.view(feature_maps.shape[0], 1, 1)\n",
        "\n",
        "    # Compute Grad-CAM heatmap\n",
        "    cam = torch.sum(pooled_gradients * feature_maps, dim=0).cpu().detach().numpy()\n",
        "    cam = np.maximum(cam, 0)  # Apply ReLU\n",
        "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))  # Resize to match original image size\n",
        "\n",
        "    # Overlay the heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam / cam.max()), cv2.COLORMAP_JET)\n",
        "    overlayed_img = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    # Display result\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(overlayed_img)\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(\"Grad-CAM Heatmap\")\n",
        "    plt.show()\n",
        "\n",
        "# Run Grad-CAM on a sample image\n",
        "generate_grad_cam(\"/content/drive/My Drive/Rename.JPG\")\n"
      ],
      "metadata": {
        "id": "Ke2TB-uI1k2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import preprocess_image as gradcam_preprocess\n",
        "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
        "\n",
        "# Load the trained Detectron2 model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Adjust based on your dataset\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/My Drive/model1_final.pth\"  # Change to your trained model path\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
        "\n",
        "# Initialize model manually for Grad-CAM\n",
        "model = build_model(cfg)\n",
        "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
        "model.eval()\n",
        "\n",
        "# Register Grad-CAM Hook\n",
        "feature_maps = None\n",
        "gradients = None\n",
        "\n",
        "def forward_hook(module, input, output):\n",
        "    global feature_maps\n",
        "    feature_maps = output\n",
        "\n",
        "def backward_hook(module, grad_in, grad_out):\n",
        "    global gradients\n",
        "    gradients = grad_out[0]\n",
        "\n",
        "# Register hooks to a deeper convolutional layer (res4 for better feature visualization)\n",
        "target_layer = model.backbone.bottom_up.res4\n",
        "target_layer.register_forward_hook(forward_hook)\n",
        "target_layer.register_full_backward_hook(backward_hook)\n",
        "\n",
        "# Initialize predictor\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Function to get 8 random images from test folder\n",
        "def get_random_images(folder, num_images=8):\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Check valid image formats\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    if len(image_paths) == 0:\n",
        "        raise ValueError(\" ERROR: No images found in test folder!\")\n",
        "\n",
        "    random.shuffle(image_paths)\n",
        "    return image_paths[:num_images]\n",
        "\n",
        "# Function to generate Grad-CAM heatmap\n",
        "def generate_grad_cam(image_path):\n",
        "    global feature_maps, gradients\n",
        "\n",
        "    torch.cuda.empty_cache()  # Free up GPU memory\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Unable to load image at '{image_path}'. Check the path.\")\n",
        "        return None, None\n",
        "\n",
        "    img = cv2.resize(img, (512, 512))  # Resize to avoid memory overflow\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Convert to tensor and send to GPU\n",
        "    img_tensor = torch.tensor(img).permute(2, 0, 1).float().unsqueeze(0).cuda()\n",
        "\n",
        "    # Forward pass\n",
        "    model.zero_grad()\n",
        "    inputs = [{\"image\": img_tensor.squeeze()}]\n",
        "    outputs = model(inputs)[0]  # Extract first output dictionary\n",
        "\n",
        "    if \"instances\" not in outputs or len(outputs[\"instances\"]) == 0:\n",
        "        print(\"No objects detected in\", image_path)\n",
        "        return img, None\n",
        "\n",
        "    instances = outputs[\"instances\"]\n",
        "    scores = instances.scores\n",
        "    highest_class_idx = scores.argmax()\n",
        "    scores[highest_class_idx].backward()\n",
        "\n",
        "    # Compute Grad-CAM heatmap\n",
        "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])  # Average over height & width\n",
        "    feature_maps = feature_maps.squeeze(0)  # Remove batch dimension\n",
        "    pooled_gradients = pooled_gradients.view(feature_maps.shape[0], 1, 1)\n",
        "\n",
        "    cam = torch.sum(pooled_gradients * feature_maps, dim=0).cpu().detach().numpy()\n",
        "    cam = np.maximum(cam, 0)  # Apply ReLU\n",
        "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))  # Resize to match original image size\n",
        "\n",
        "    # Overlay the heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam / cam.max()), cv2.COLORMAP_JET)\n",
        "    overlayed_img = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    return img, overlayed_img  # Return original & heatmap image\n",
        "\n",
        "# Run Grad-CAM on 8 Random Images & Display in Grid\n",
        "test_folder = \"/content/drive/My Drive/test\"  # Change this to your test directory\n",
        "selected_images = get_random_images(test_folder, num_images=8)\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 2x4 grid for displaying images\n",
        "\n",
        "for i, img_path in enumerate(selected_images):\n",
        "    row, col = divmod(i, 4)\n",
        "\n",
        "    original, heatmap = generate_grad_cam(img_path)\n",
        "    if heatmap is None:\n",
        "        continue  # Skip images with no detections\n",
        "\n",
        "    # Display results\n",
        "    axes[row, col].imshow(heatmap)\n",
        "    axes[row, col].set_title(f\"Image {i+1}\")\n",
        "    axes[row, col].axis(\"off\")\n",
        "\n",
        "# Show all Grad-CAM results\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "w6NS7HFc1o1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PBE"
      ],
      "metadata": {
        "id": "slGzqPv01tPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "\n",
        "# Load the trained Detectron2 model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Adjust based on your dataset\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/My Drive/model1_final.pth\"  # Change to your trained model path\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
        "\n",
        "# Initialize model\n",
        "model = build_model(cfg)\n",
        "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
        "model.eval()\n",
        "\n",
        "# Initialize predictor for normal inference\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Function to get 8 random images from test folder\n",
        "def get_random_images(folder, num_images=8):\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Check valid image formats\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    if len(image_paths) == 0:\n",
        "        raise ValueError(\"ERROR: No images found in test folder!\")\n",
        "\n",
        "    random.shuffle(image_paths)\n",
        "    return image_paths[:num_images]\n",
        "\n",
        "# Occlusion Sensitivity Function\n",
        "def perturb_image(image, mask_size=50, stride=25):\n",
        "    \"\"\"\n",
        "    Iteratively occludes different parts of the image and measures impact on predictions.\n",
        "    Generates an importance heatmap showing regions most sensitive to occlusion.\n",
        "    \"\"\"\n",
        "    h, w, _ = image.shape\n",
        "    heatmap = np.zeros((h, w))\n",
        "\n",
        "    # Run original prediction\n",
        "    original_output = predictor(image)\n",
        "    original_score = original_output[\"instances\"].scores.max().item() if len(original_output[\"instances\"]) > 0 else 0\n",
        "\n",
        "    # Slide occlusion window across image\n",
        "    for y in range(0, h, stride):\n",
        "        for x in range(0, w, stride):\n",
        "            # Create occlusion mask\n",
        "            occluded_image = image.copy()\n",
        "            occluded_image[y:y+mask_size, x:x+mask_size] = 0  # Black-out occluded area\n",
        "\n",
        "            # Run prediction on occluded image\n",
        "            occluded_output = predictor(occluded_image)\n",
        "            occluded_score = occluded_output[\"instances\"].scores.max().item() if len(occluded_output[\"instances\"]) > 0 else 0\n",
        "\n",
        "            # Compute score drop (higher = more important)\n",
        "            heatmap[y:y+mask_size, x:x+mask_size] = original_score - occluded_score\n",
        "\n",
        "    # Normalize heatmap\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "    return heatmap\n",
        "\n",
        "# Run Perturbation-Based Explainability on 8 Random Images & Display in Grid\n",
        "test_folder = \"/content/drive/My Drive/test\"  # Change this to your test directory\n",
        "selected_images = get_random_images(test_folder, num_images=8)\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 2x4 grid for displaying images\n",
        "\n",
        "for i, img_path in enumerate(selected_images):\n",
        "    row, col = divmod(i, 4)\n",
        "\n",
        "    # Load image\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"ERROR: Could not load {img_path}\")\n",
        "        continue\n",
        "    img = cv2.resize(img, (512, 512))  # Resize for consistency\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "\n",
        "    # Compute occlusion sensitivity heatmap\n",
        "    heatmap = perturb_image(img)\n",
        "\n",
        "    # Convert to color heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    overlayed_img = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    # Display results\n",
        "    axes[row, col].imshow(overlayed_img)\n",
        "    axes[row, col].set_title(f\"Image {i+1}\")\n",
        "    axes[row, col].axis(\"off\")\n",
        "\n",
        "# Show all perturbation-based results\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ZjX-kv-l11mh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}