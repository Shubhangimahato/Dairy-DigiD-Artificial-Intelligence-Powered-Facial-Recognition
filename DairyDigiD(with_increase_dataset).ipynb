{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/w4gmz9J/GFO1GqyYZ+sB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shubhangimahato/Dairy-DigiD-Artificial-Intelligence-Powered-Facial-Recognition/blob/main/DairyDigiD(with_increase_dataset).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwnohH67X1KG"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU is available\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"GPU is not available\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show pyyaml\n"
      ],
      "metadata": {
        "id": "tTs_LEqOYdKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "metadata": {
        "id": "cUKLO71LYdMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !nvidia-smi\n",
        "!nvcc --version #10.1\n",
        "!python --version # 3.7.6"
      ],
      "metadata": {
        "id": "UMZG6-L-YdOu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "g85Unm8wYdQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torchvision\n",
        "print(torch.__version__, torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "N0y1G6WIYdTa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Detectron 2"
      ],
      "metadata": {
        "id": "fUrev9FxYyXq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install \"git+https://github.com/facebookresearch/detectron2.git\""
      ],
      "metadata": {
        "id": "5oUjGX0wYdWW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "import detectron2\n",
        "from detectron2.utils.logger import setup_logger\n",
        "setup_logger()\n",
        "\n",
        "import numpy as np\n",
        "import os, json, cv2, random\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.structures import BoxMode\n"
      ],
      "metadata": {
        "id": "duQl-pl5YdZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Dataset"
      ],
      "metadata": {
        "id": "v44yVZNOY9Pr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "# Define the root directories for both '28 May' and '24 June'\n",
        "root_dirs = {\n",
        "    '28 May': '/content/drive/My Drive/28 May_00101/28 May/',\n",
        "    '24 June': '/content/drive/My Drive/24 June/'\n",
        "}\n",
        "\n",
        "# Define the subdirectories to include for both '28 May' and '24 June'\n",
        "subdirs_to_include = {\n",
        "    '28 May': ['28 May_001', '28 May_002'],\n",
        "    '24 June': ['24_june_001', '24_june_002', '24_june_003', '24_june_004']\n",
        "}\n",
        "\n",
        "# Initialize lists to collect all images and annotations\n",
        "all_annotations = []\n",
        "\n",
        "# Loop through the root directories and their specified subdirectories\n",
        "for root_dir_name, subdirs in subdirs_to_include.items():\n",
        "    root_dir = root_dirs[root_dir_name]\n",
        "    for subdir_name in subdirs:\n",
        "        subdir_path = os.path.join(root_dir, subdir_name)\n",
        "        annotation_folder = os.path.join(subdir_path, 'annotations')\n",
        "        images_root_folder = os.path.join(subdir_path, 'images')\n",
        "\n",
        "        # Load all JSON annotation files from the annotation folder\n",
        "        for annotation_file in os.listdir(annotation_folder):\n",
        "            if annotation_file.endswith('.json'):\n",
        "                json_path = os.path.join(annotation_folder, annotation_file)\n",
        "                with open(json_path, 'r') as f:\n",
        "                    annotations = json.load(f)\n",
        "                    for annotation in annotations['annotations']:\n",
        "                        annotation['image_id'] = f\"{subdir_name}_{annotation['image_id']}\"\n",
        "                    for image_info in annotations['images']:\n",
        "                        image_info['id'] = f\"{subdir_name}_{image_info['id']}\"\n",
        "\n",
        "                        # Remove redundant folder name from 'file_name' if present\n",
        "                        if image_info['file_name'].startswith(f\"{root_dir_name}/\"):\n",
        "                            image_info['file_name'] = image_info['file_name'][len(root_dir_name) + 1:]\n",
        "\n",
        "                        # Now combine with 'images_root_folder'\n",
        "                        image_info['file_name'] = os.path.join(images_root_folder, image_info['file_name'])\n",
        "\n",
        "                    all_annotations.append({\n",
        "                        'images': annotations['images'],\n",
        "                        'annotations': annotations['annotations']\n",
        "                    })\n",
        "\n",
        "# Now create the dataset dictionary\n",
        "def custom_cattle_dataset():\n",
        "    dataset_dicts = []\n",
        "    for annotation_group in all_annotations:\n",
        "        images_info_map = {img_info['id']: img_info for img_info in annotation_group['images']}\n",
        "        for annotation in annotation_group['annotations']:\n",
        "            record = {}\n",
        "            image_info = images_info_map.get(annotation['image_id'])\n",
        "            if image_info is None:\n",
        "                print(f\"Image info not found for image ID: {annotation['image_id']}\")\n",
        "                continue\n",
        "            record['file_name'] = image_info['file_name']\n",
        "            record['image_id'] = image_info['id']\n",
        "            record['height'] = image_info['height']\n",
        "            record['width'] = image_info['width']\n",
        "            # Determine category based on file path\n",
        "            if \"2 Years old or less\" in record[\"file_name\"] or \"Heifer\" in record[\"file_name\"]:\n",
        "                category_id = 0\n",
        "            elif \"Dry Cows\" in record[\"file_name\"] or \"dry cows\" in record[\"file_name\"]:\n",
        "                category_id = 1\n",
        "            elif \"Mature Milking Cow\" in record[\"file_name\"]:\n",
        "                category_id = 2\n",
        "            elif \"Pregnant\" in record[\"file_name\"]:\n",
        "                category_id = 3\n",
        "            else:\n",
        "                category_id = 999  # Undefined category (optional)\n",
        "            obj = {\n",
        "                'bbox': annotation['bbox'],\n",
        "                'bbox_mode': BoxMode.XYWH_ABS,\n",
        "                'category_id': category_id,\n",
        "                'segmentation': annotation.get('segmentation', []),\n",
        "                'keypoints': annotation.get('keypoints', []),\n",
        "                'iscrowd': annotation.get('iscrowd', 0),\n",
        "            }\n",
        "            record['annotations'] = [obj]\n",
        "            dataset_dicts.append(record)\n",
        "    return dataset_dicts\n",
        "\n",
        "# Generate the dataset dictionary\n",
        "dataset_dicts = custom_cattle_dataset()\n",
        "print(\"Dataset Dicts Sample:\", dataset_dicts[:5])\n"
      ],
      "metadata": {
        "id": "KpWgBoOOYdbj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the keypoint flip map using numerical points\n",
        "keypoint_flip_map = [\n",
        "    (1, 5),   # Left Eye extreme right <-> Right Eye extreme left\n",
        "    (2, 6),   # Left Eye extreme left <-> Right Eye extreme righta\n",
        "    (3, 7),   # Left Eye extreme top <-> Right Eye extreme top\n",
        "    (4, 8),   # Left Eye extreme bottom <-> Right Eye extreme bottom\n",
        "    (9, 14),  # Left Ear extreme top right <-> Right Ear extreme top left\n",
        "    (10, 15), # Left Ear extreme left <-> Right Ear extreme right\n",
        "    (11, 16), # Left Ear extreme top mid <-> Right Ear extreme top mid\n",
        "    (12, 17), # Left Ear extreme bottom mid <-> Right Ear extreme bottom mid\n",
        "    (13, 18), # Left Ear extreme bottom right <-> Right Ear extreme bottom left\n",
        "    (19, 21), # Muzzle Top left <-> Muzzle Top right\n",
        "    (20, 20), # Muzzle Top mid <-> Muzzle Top mid (no change)\n",
        "    (21, 19), # Muzzle Top right <-> Muzzle Top left\n",
        "    (22, 24), # Muzzle Bottom right <-> Muzzle Bottom left\n",
        "    (23, 23), # Muzzle Bottom mid <-> Muzzle Bottom mid (no change)\n",
        "    (24, 22), # Muzzle Bottom left <-> Muzzle Bottom right\n",
        "    (28, 30), # Head left side <-> Head right side\n",
        "    (29, 29), # Head extreme top <-> Head extreme top (no change)\n",
        "    (30, 28), # Head right side <-> Head left side\n",
        "]\n",
        "\n",
        "# Define the keypoint names for 30 points\n",
        "keypoint_names = [\n",
        "    'kp1', 'kp2', 'kp3', 'kp4', 'kp5', 'kp6', 'kp7', 'kp8', 'kp9', 'kp10',\n",
        "    'kp11', 'kp12', 'kp13', 'kp14', 'kp15', 'kp16', 'kp17', 'kp18', 'kp19', 'kp20',\n",
        "    'kp21', 'kp22', 'kp23', 'kp24', 'kp25', 'kp26', 'kp27', 'kp28', 'kp29', 'kp30'\n",
        "]"
      ],
      "metadata": {
        "id": "EEQcEPluYdeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import random\n",
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "# Function to split dataset into train, val, and test sets\n",
        "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15, seed=42):\n",
        "    \"\"\"\n",
        "    Splits the dataset into training, validation, and test sets.\n",
        "    \"\"\"\n",
        "    assert train_ratio + val_ratio + test_ratio == 1, \"Ratios must sum up to 1!\"\n",
        "\n",
        "    # Set random seed for reproducibility\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Shuffle dataset\n",
        "    shuffled_dataset = random.sample(dataset, len(dataset))\n",
        "\n",
        "    # Compute split indices\n",
        "    total_size = len(dataset)\n",
        "    train_size = int(total_size * train_ratio)\n",
        "    val_size = int(total_size * val_ratio)\n",
        "\n",
        "    # Split dataset\n",
        "    train_set = shuffled_dataset[:train_size]\n",
        "    val_set = shuffled_dataset[train_size:train_size + val_size]\n",
        "    test_set = shuffled_dataset[train_size + val_size:]\n",
        "\n",
        "    return train_set, val_set, test_set\n",
        "\n",
        "# Ensure your dataset is already loaded in `dataset_dicts`\n",
        "train_set, val_set, test_set = split_dataset(dataset_dicts)\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Train Set Size: {len(train_set)}\")\n",
        "print(f\"Validation Set Size: {len(val_set)}\")\n",
        "print(f\"Test Set Size: {len(test_set)}\")\n",
        "\n",
        "# Define functions to return datasets\n",
        "def get_cows_train():\n",
        "    return train_set\n",
        "\n",
        "def get_cows_val():\n",
        "    return val_set\n",
        "\n",
        "def get_cows_test():\n",
        "    return test_set\n",
        "\n",
        "# Register datasets\n",
        "DatasetCatalog.register(\"cows_train\", get_cows_train)\n",
        "DatasetCatalog.register(\"cows_val\", get_cows_val)\n",
        "DatasetCatalog.register(\"cows_test\", get_cows_test)\n",
        "\n",
        "# Define common metadata\n",
        "metadata_info = {\n",
        "    \"keypoint_flip_map\": keypoint_flip_map,\n",
        "    \"keypoint_names\": keypoint_names,\n",
        "    \"thing_classes\": [\"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"],\n",
        "    \"evaluator_type\": \"coco\"\n",
        "}\n",
        "\n",
        "# Apply metadata to all datasets\n",
        "for dataset_name in [\"cows_train\", \"cows_val\", \"cows_test\"]:\n",
        "    MetadataCatalog.get(dataset_name).keypoint_flip_map = metadata_info[\"keypoint_flip_map\"]\n",
        "    MetadataCatalog.get(dataset_name).keypoint_names = metadata_info[\"keypoint_names\"]\n",
        "    MetadataCatalog.get(dataset_name).thing_classes = metadata_info[\"thing_classes\"]\n",
        "    MetadataCatalog.get(dataset_name).evaluator_type = metadata_info[\"evaluator_type\"]\n",
        "\n",
        "print(\"Datasets registered successfully with metadata!\")\n",
        "\n"
      ],
      "metadata": {
        "id": "_ZsU0XCsYdg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing"
      ],
      "metadata": {
        "id": "eCU5U-MHZOhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.modeling import build_model\n",
        "from detectron2.modeling.meta_arch import GeneralizedRCNN\n",
        "from detectron2.modeling.roi_heads import StandardROIHeads\n",
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "\n",
        "class FocalLossRCNN(GeneralizedRCNN):\n",
        "    def forward(self, batched_inputs):\n",
        "        if self.training:\n",
        "            losses = super().forward(batched_inputs)\n",
        "            # Modify losses[\"loss_cls\"] with a custom focal loss\n",
        "            losses[\"loss_cls\"] = focal_loss_function(self.pred_class_logits, self.gt_classes)\n",
        "            return losses\n",
        "        else:\n",
        "            return super().forward(batched_inputs)\n",
        "\n",
        "# Integrate into Trainer\n",
        "cfg.MODEL.ROI_HEADS.LOSS = \"FocalLossRCNN\"\n"
      ],
      "metadata": {
        "id": "E3OnputgYdju"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.BOX_LOSS_TYPE = \"siou\"  # Custom loss integration\n",
        "cfg.SOLVER.OPTIMIZER = \"AdamW\"\n",
        "cfg.SOLVER.BASE_LR = 0.0001  # Adjust learning rate for AdamW\n"
      ],
      "metadata": {
        "id": "1tmwAY4gYdmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data.samplers import RepeatFactorTrainingSampler\n",
        "\n",
        "def build_train_loader(cfg):\n",
        "    dataset_dicts = DatasetCatalog.get(cfg.DATASETS.TRAIN[0])\n",
        "    repeat_factors = RepeatFactorTrainingSampler.repeat_factors_from_category_frequency(\n",
        "        dataset_dicts, repeat_thresh=0.1\n",
        "    )\n",
        "    return build_detection_train_loader(cfg, sampler=RepeatFactorTrainingSampler(repeat_factors))\n"
      ],
      "metadata": {
        "id": "tgGj7TtQYdoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "\n",
        "cfg = get_cfg()\n",
        "cfg.MODEL.BACKBONE.NAME = \"build_resnet_fpn_backbone\"\n",
        "cfg.MODEL.RESNETS.DEPTH = 101\n",
        "cfg.MODEL.RESNETS.OUT_FEATURES = [\"res2\", \"res3\", \"res4\", \"res5\"]\n"
      ],
      "metadata": {
        "id": "qpRgdzb7Ydri"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.FPN.NORM = \"GN\"  # Group Normalization for FPN\n",
        "cfg.MODEL.FPN.IN_FEATURES = [\"p2\", \"p3\", \"p4\", \"p5\"]  # Include lower and higher layers\n"
      ],
      "metadata": {
        "id": "RjeUctekYduT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.BACKBONE.ATTENTION_TYPE = \"CoordinateAttention\"  # Requires custom implementation\n"
      ],
      "metadata": {
        "id": "UqZSSYb5Ydw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cfg.MODEL.ANCHOR_GENERATOR.SIZES = [[32, 64, 128, 256, 512]]  # Adjust based on dataset\n",
        "cfg.MODEL.ANCHOR_GENERATOR.ASPECT_RATIOS = [[0.5, 1.0, 2.0]]  # Common ratios\n"
      ],
      "metadata": {
        "id": "apNTNb8LYdzp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train, val and test"
      ],
      "metadata": {
        "id": "P5xDO8juZsCZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "import numpy as np\n",
        "from detectron2.structures import Boxes, Instances, Keypoints\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
        "from detectron2.data import build_detection_train_loader\n",
        "\n",
        "# Enable expandable memory allocation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Configuration setup\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"cows_train_balanced\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "\n",
        "cfg.DATALOADER.NUM_WORKERS = 2\n",
        "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")\n",
        "\n",
        "cfg.SOLVER.MAX_ITER = 2000\n",
        "cfg.SOLVER.IMS_PER_BATCH = 1\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 16\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4\n",
        "cfg.MODEL.ROI_KEYPOINT_HEAD.NUM_KEYPOINTS = 30\n",
        "cfg.TEST.KEYPOINT_OKS_SIGMAS = np.ones((30, 1), dtype=float).tolist()\n",
        "cfg.SOLVER.STEPS = [3000, 3800]\n",
        "cfg.SOLVER.GAMMA = 0.1\n",
        "\n",
        "# Fix Learning Rate and Gradient Clipping\n",
        "cfg.SOLVER.BASE_LR = 0.0005  # Lowered to prevent NaN errors\n",
        "cfg.SOLVER.WARMUP_FACTOR = 0.0001\n",
        "cfg.SOLVER.WARMUP_ITERS = 500\n",
        "cfg.SOLVER.CLIP_GRADIENTS.ENABLED = True\n",
        "cfg.SOLVER.CLIP_GRADIENTS.CLIP_TYPE = \"value\"\n",
        "cfg.SOLVER.CLIP_GRADIENTS.CLIP_VALUE = 1.0  # Limits large gradient updates\n",
        "\n",
        "# Define keypoint flip map and keypoint names\n",
        "keypoint_flip_map = [(1, 5), (2, 6), (3, 7), (4, 8), (9, 14), (10, 15), (11, 16), (12, 17),\n",
        "                     (13, 18), (19, 21), (20, 20), (21, 19), (22, 24), (23, 23), (24, 22),\n",
        "                     (28, 30), (29, 29), (30, 28)]\n",
        "keypoint_names = [f'kp{i}' for i in range(1, 31)]\n",
        "\n",
        "MetadataCatalog.get(\"cows_train_balanced\").keypoint_flip_map = keypoint_flip_map\n",
        "MetadataCatalog.get(\"cows_train_balanced\").keypoint_names = keypoint_names\n",
        "MetadataCatalog.get(\"cows_train_balanced\").thing_classes = [\"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"]\n",
        "MetadataCatalog.get(\"cows_train_balanced\").evaluator_type = \"coco\"\n",
        "\n",
        "# Function to check file existence\n",
        "def check_alternative_extensions(file_path):\n",
        "    base, _ = os.path.splitext(file_path)\n",
        "    extensions = [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\", \".png\"]\n",
        "    for ext in extensions:\n",
        "        new_path = base + ext\n",
        "        if os.path.exists(new_path):\n",
        "            return new_path\n",
        "    return None\n",
        "\n",
        "def filter_missing_files(dataset_dicts):\n",
        "    \"\"\"Filters missing files and corrects paths with case mismatches.\"\"\"\n",
        "    filtered_dataset = []\n",
        "    for record in dataset_dicts:\n",
        "        file_path = record[\"file_name\"]\n",
        "        if os.path.exists(file_path):\n",
        "            filtered_dataset.append(record)\n",
        "        else:\n",
        "            corrected_path = check_alternative_extensions(file_path)\n",
        "            if corrected_path:\n",
        "                record[\"file_name\"] = corrected_path\n",
        "                filtered_dataset.append(record)\n",
        "    return filtered_dataset\n",
        "\n",
        "# Load and filter dataset\n",
        "dataset_dicts = DatasetCatalog.get(\"cows_train\")\n",
        "filtered_dataset_dicts = filter_missing_files(dataset_dicts)\n",
        "\n",
        "# Oversample \"Pregnant\" class\n",
        "pregnant_data = [d for d in filtered_dataset_dicts if d[\"annotations\"][0][\"category_id\"] == 3]\n",
        "augmented_pregnant_data = []\n",
        "for d in pregnant_data:\n",
        "    for _ in range(30):  # Augment each sample 30 times\n",
        "        aug_data = d.copy()\n",
        "        aug_data[\"file_name\"] = d[\"file_name\"]\n",
        "        augmented_pregnant_data.append(aug_data)\n",
        "\n",
        "balanced_dataset_dicts = filtered_dataset_dicts + augmented_pregnant_data\n",
        "\n",
        "def get_balanced_cows_train():\n",
        "    return balanced_dataset_dicts\n",
        "\n",
        "if \"cows_train_balanced\" in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(\"cows_train_balanced\")\n",
        "    MetadataCatalog.remove(\"cows_train_balanced\")\n",
        "\n",
        "DatasetCatalog.register(\"cows_train_balanced\", get_balanced_cows_train)\n",
        "\n",
        "# Augmentation function for \"Pregnant\" class\n",
        "def pregnant_extra_augmentations(image, keypoints):\n",
        "    transform_list = [\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.RandomBrightnessContrast(p=0.5),\n",
        "        A.MotionBlur(p=0.5),\n",
        "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.5),\n",
        "        A.Rotate(limit=10, p=0.5, border_mode=cv2.BORDER_REFLECT),\n",
        "        ToTensorV2()\n",
        "    ]\n",
        "    aug = A.Compose(transform_list, keypoint_params=A.KeypointParams(format=\"xy\", remove_invisible=True))\n",
        "    augmented = aug(image=image, keypoints=keypoints)\n",
        "    return augmented[\"image\"].permute(1, 2, 0).numpy(), augmented[\"keypoints\"]\n",
        "\n",
        "# Custom Mapper\n",
        "def custom_mapper(dataset_dict):\n",
        "    dataset_dict = dataset_dict.copy()\n",
        "    image = cv2.imread(dataset_dict[\"file_name\"])\n",
        "    if image is None:\n",
        "        return None\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    category_id = dataset_dict[\"annotations\"][0][\"category_id\"]\n",
        "    keypoints = [ann.get(\"keypoints\", []) for ann in dataset_dict[\"annotations\"]]\n",
        "    if category_id == 3:\n",
        "        image, keypoints = pregnant_extra_augmentations(image, keypoints)\n",
        "\n",
        "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1))\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "    instances = Instances((height, width))\n",
        "\n",
        "    boxes = [ann[\"bbox\"] for ann in dataset_dict[\"annotations\"]]\n",
        "    boxes = np.array(boxes, dtype=np.float32)\n",
        "    boxes = BoxMode.convert(boxes, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS)\n",
        "\n",
        "    valid_boxes = []\n",
        "    valid_classes = []\n",
        "    valid_keypoints = []\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        if (x2 - x1) > 0 and (y2 - y1) > 0:\n",
        "            valid_boxes.append(box)\n",
        "            valid_classes.append(dataset_dict[\"annotations\"][i][\"category_id\"])\n",
        "            if keypoints[i]:\n",
        "                valid_keypoints.append(keypoints[i])\n",
        "\n",
        "    if len(valid_boxes) == 0:\n",
        "        return None\n",
        "\n",
        "    instances.gt_boxes = Boxes(torch.as_tensor(valid_boxes, dtype=torch.float32))\n",
        "    instances.gt_classes = torch.as_tensor(valid_classes, dtype=torch.int64)\n",
        "\n",
        "    if valid_keypoints:\n",
        "        valid_keypoints = np.array(valid_keypoints, dtype=np.float32).reshape(len(valid_keypoints), -1, 3)\n",
        "        instances.gt_keypoints = Keypoints(torch.as_tensor(valid_keypoints, dtype=torch.float32))\n",
        "\n",
        "    dataset_dict[\"instances\"] = instances\n",
        "    return dataset_dict\n",
        "\n",
        "class CustomTrainer(DefaultTrainer):\n",
        "    @classmethod\n",
        "    def build_train_loader(cls, cfg):\n",
        "        return build_detection_train_loader(cfg, mapper=custom_mapper)\n",
        "\n",
        "cfg.OUTPUT_DIR = \"./my_custom_output_dir\"\n",
        "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "trainer = CustomTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "u62aaKmgYd2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Set the path to the metrics.json file\n",
        "output_dir = cfg.OUTPUT_DIR\n",
        "metrics_file = os.path.join(output_dir, \"metrics.json\")\n",
        "\n",
        "# Load the metrics from the JSON file\n",
        "metrics = []\n",
        "with open(metrics_file, 'r') as f:\n",
        "    for line in f:\n",
        "        metrics.append(json.loads(line))\n",
        "\n",
        "# Extract the iteration values, training loss, and validation loss\n",
        "iterations = [x['iteration'] for x in metrics if 'total_loss' in x]\n",
        "train_losses = [x['total_loss'] for x in metrics if 'total_loss' in x]\n",
        "\n",
        "# Validation loss may have different key, make sure to check the key in your JSON\n",
        "val_iterations = [x['iteration'] for x in metrics if 'validation_loss' in x]\n",
        "val_losses = [x['validation_loss'] for x in metrics if 'validation_loss' in x]\n",
        "\n",
        "# Plot both the training and validation loss curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(iterations, train_losses, label='Training Loss', color='blue')\n",
        "if len(val_iterations) > 0 and len(val_losses) > 0:\n",
        "    plt.plot(val_iterations, val_losses, label='Validation Loss', color='red')\n",
        "\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss over Iterations')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7S2XFUcHYd46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify source and new destination paths\n",
        "source_path = \"./my_custom_output_dir/model_final.pth\"\n",
        "destination_path = \"/content/drive/My Drive/model2_final.pth\"  # Change this to the desired path in your Google Drive\n",
        "\n",
        "# Copy the model file using shutil\n",
        "shutil.copy(source_path, destination_path)\n",
        "\n",
        "print(f\"Model copied to {destination_path}\")\n"
      ],
      "metadata": {
        "id": "Rw5poN-2Yd7v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
        "from detectron2.structures import Boxes, Instances, Keypoints\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "\n",
        "# Function to check file existence\n",
        "def check_alternative_extensions(file_path):\n",
        "    base, _ = os.path.splitext(file_path)\n",
        "    extensions = [\".jpg\", \".jpeg\", \".JPG\", \".JPEG\", \".png\"]\n",
        "    for ext in extensions:\n",
        "        new_path = base + ext\n",
        "        if os.path.exists(new_path):\n",
        "            return new_path\n",
        "    return None\n",
        "\n",
        "def filter_missing_files(dataset_dicts):\n",
        "    \"\"\"Filters missing files and corrects paths with case mismatches.\"\"\"\n",
        "    filtered_dataset = []\n",
        "    for record in dataset_dicts:\n",
        "        file_path = record[\"file_name\"]\n",
        "        if os.path.exists(file_path):\n",
        "            filtered_dataset.append(record)\n",
        "        else:\n",
        "            corrected_path = check_alternative_extensions(file_path)\n",
        "            if corrected_path:\n",
        "                record[\"file_name\"] = corrected_path\n",
        "                filtered_dataset.append(record)\n",
        "    return filtered_dataset\n",
        "\n",
        "# Load and filter validation dataset\n",
        "dataset_val = DatasetCatalog.get(\"cows_val\")\n",
        "filtered_val = filter_missing_files(dataset_val)\n",
        "\n",
        "# Register validation dataset\n",
        "def get_balanced_val():\n",
        "    return filtered_val\n",
        "\n",
        "if \"cows_val_balanced\" in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(\"cows_val_balanced\")\n",
        "    MetadataCatalog.remove(\"cows_val_balanced\")\n",
        "\n",
        "DatasetCatalog.register(\"cows_val_balanced\", get_balanced_val)\n",
        "MetadataCatalog.get(\"cows_val_balanced\").thing_classes = [\"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"]\n",
        "\n",
        "# Define validation data mapper (NO AUGMENTATION)\n",
        "def custom_val_mapper(dataset_dict):\n",
        "    dataset_dict = dataset_dict.copy()\n",
        "    image = cv2.imread(dataset_dict[\"file_name\"])\n",
        "    if image is None:\n",
        "        return None\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1))\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "    instances = Instances((height, width))\n",
        "\n",
        "    boxes = [ann[\"bbox\"] for ann in dataset_dict[\"annotations\"]]\n",
        "    boxes = np.array(boxes, dtype=np.float32)\n",
        "    boxes = Boxes(torch.as_tensor(BoxMode.convert(boxes, BoxMode.XYWH_ABS, BoxMode.XYXY_ABS), dtype=torch.float32))\n",
        "\n",
        "    instances.gt_boxes = boxes\n",
        "    instances.gt_classes = torch.as_tensor([ann[\"category_id\"] for ann in dataset_dict[\"annotations\"]], dtype=torch.int64)\n",
        "\n",
        "    if \"keypoints\" in dataset_dict[\"annotations\"][0]:\n",
        "        keypoints = [ann[\"keypoints\"] for ann in dataset_dict[\"annotations\"]]\n",
        "        keypoints = np.array(keypoints, dtype=np.float32).reshape(len(keypoints), -1, 3)\n",
        "        instances.gt_keypoints = Keypoints(torch.as_tensor(keypoints, dtype=torch.float32))\n",
        "\n",
        "    dataset_dict[\"instances\"] = instances\n",
        "    return dataset_dict\n",
        "\n",
        "# Run validation evaluation\n",
        "val_loader = build_detection_test_loader(cfg, \"cows_val_balanced\", mapper=custom_val_mapper)\n",
        "evaluator = COCOEvaluator(\"cows_val_balanced\", cfg, False, output_dir=\"./output/\")\n",
        "print(\"üîç Evaluating on Validation Set...\")\n",
        "inference_on_dataset(trainer.model, val_loader, evaluator)\n"
      ],
      "metadata": {
        "id": "uD8i60UIYd-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
        "from detectron2.structures import Boxes, Instances, Keypoints\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2.utils.visualizer import Visualizer\n",
        "from detectron2.data.detection_utils import convert_image_to_rgb\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# Define categories\n",
        "categories = [\"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"]\n",
        "\n",
        "# Ensure dataset is available and filter missing files\n",
        "dataset_test = DatasetCatalog.get(\"cows_test\")\n",
        "filtered_test = [d for d in dataset_test if os.path.exists(d[\"file_name\"])]\n",
        "\n",
        "# Register filtered testing dataset\n",
        "def get_test_data():\n",
        "    return filtered_test\n",
        "\n",
        "if \"cows_test_balanced\" in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(\"cows_test_balanced\")\n",
        "    MetadataCatalog.remove(\"cows_test_balanced\")\n",
        "\n",
        "DatasetCatalog.register(\"cows_test_balanced\", get_test_data)\n",
        "MetadataCatalog.get(\"cows_test_balanced\").thing_classes = categories\n",
        "\n",
        "# Load model weights and setup predictor\n",
        "cfg.MODEL.WEIGHTS = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2  # Adjust confidence threshold\n",
        "cfg.DATASETS.TEST = (\"cows_test_balanced\",)\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Lists to store predictions\n",
        "predicted_labels = []\n",
        "actual_labels = []\n",
        "\n",
        "# Extract ground truth from dataset\n",
        "def extract_ground_truth(data):\n",
        "    \"\"\"Extracts actual labels from dataset.\"\"\"\n",
        "    return MetadataCatalog.get(\"cows_test_balanced\").thing_classes[data[\"annotations\"][0][\"category_id\"]]\n",
        "\n",
        "# Process images for evaluation\n",
        "def process_images_for_evaluation(output_folder):\n",
        "    \"\"\"Evaluate model predictions against ground truth.\"\"\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    for data in filtered_test:\n",
        "        file_path = data[\"file_name\"]\n",
        "        image = cv2.imread(file_path)\n",
        "\n",
        "        if image is None:\n",
        "            print(f\"Error loading image: {file_path}\")\n",
        "            continue\n",
        "\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Run inference\n",
        "        outputs = predictor(image)\n",
        "\n",
        "        # Get predicted class labels\n",
        "        predicted_class_indices = outputs[\"instances\"].pred_classes.tolist()\n",
        "        predicted_classes = [categories[idx] for idx in predicted_class_indices if idx < len(categories)]\n",
        "\n",
        "        # Get actual class\n",
        "        actual_class = extract_ground_truth(data)\n",
        "\n",
        "        # If at least one correct prediction, count as correct\n",
        "        if any(pred == actual_class for pred in predicted_classes):\n",
        "            final_pred = actual_class\n",
        "        else:\n",
        "            final_pred = predicted_classes[0] if predicted_classes else \"Unknown\"\n",
        "\n",
        "        # Store results\n",
        "        actual_labels.append(actual_class)\n",
        "        predicted_labels.append(final_pred)\n",
        "\n",
        "        print(f\"Image: {file_path}, Actual: {actual_class}, Predicted: {predicted_classes} -> Final Assigned: {final_pred}\")\n",
        "\n",
        "# Run evaluation\n",
        "output_folder = '/content/drive/My Drive/test_results'\n",
        "process_images_for_evaluation(output_folder)\n",
        "\n",
        "# Analyze results\n",
        "print(\"\\nUnique Actual Labels:\", set(actual_labels))\n",
        "print(\"Actual Labels Distribution:\", Counter(actual_labels))\n",
        "print(\"\\nUnique Predicted Labels:\", set(predicted_labels))\n",
        "print(\"Predicted Labels Distribution:\", Counter(predicted_labels))\n",
        "\n",
        "# Generate confusion matrix and classification report\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "conf_matrix = confusion_matrix(actual_labels, predicted_labels, labels=categories)\n",
        "print(conf_matrix)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(actual_labels, predicted_labels, target_names=categories, zero_division=1))\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=categories, yticklabels=categories)\n",
        "plt.xlabel(\"Predicted Labels\")\n",
        "plt.ylabel(\"Actual Labels\")\n",
        "plt.title(\"Confusion Matrix for Cow Classification\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lndKkyxJYeBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Ensure `actual_labels` and `predicted_labels` exist üö®\n",
        "if not actual_labels or not predicted_labels:\n",
        "    print(\"Error: The actual_labels and predicted_labels lists are empty. Run model evaluation first!\")\n",
        "else:\n",
        "    # Convert labels to one-hot encoding for AUC-ROC computation\n",
        "    actual_binarized = label_binarize(actual_labels, classes=categories)\n",
        "    predicted_binarized = label_binarize(predicted_labels, classes=categories)\n",
        "\n",
        "    # Compute and Plot AUC-ROC Curve\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    for i, category in enumerate(categories):\n",
        "        fpr, tpr, _ = roc_curve(actual_binarized[:, i], predicted_binarized[:, i])\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "        plt.plot(fpr, tpr, label=f\"{category} (AUC = {roc_auc:.2f})\")\n",
        "\n",
        "    # Plot Diagonal Reference Line\n",
        "    plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
        "    plt.xlabel(\"False Positive Rate\")\n",
        "    plt.ylabel(\"True Positive Rate\")\n",
        "    plt.title(\"AUC-ROC Curve for Cow Classification\")\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "rbYDBA6eYeDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Brunswick Data"
      ],
      "metadata": {
        "id": "G1XRoUDlaELB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from detectron2.structures import BoxMode\n",
        "\n",
        "def create_labels_dict(dataset_root):\n",
        "    \"\"\"\n",
        "    Dynamically assign labels based on folder names.\n",
        "    \"\"\"\n",
        "    labels_dict = {}\n",
        "    label_id = 0\n",
        "\n",
        "    # Iterate through farms\n",
        "    for farm in os.listdir(dataset_root):\n",
        "        farm_path = os.path.join(dataset_root, farm)\n",
        "        if os.path.isdir(farm_path):  # Ensure it's a folder\n",
        "            for category in os.listdir(farm_path):  # Iterate over categories\n",
        "                category_path = os.path.join(farm_path, category)\n",
        "                if os.path.isdir(category_path) and category not in labels_dict:\n",
        "                    labels_dict[category] = label_id\n",
        "                    label_id += 1  # Increment for next category\n",
        "\n",
        "    return labels_dict\n"
      ],
      "metadata": {
        "id": "GRc4PSDUYeGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_newbrunswick_dataset(image_root):\n",
        "    \"\"\"\n",
        "    Convert New Brunswick dataset into Detectron2 format.\n",
        "    Uses dynamically created category labels.\n",
        "    \"\"\"\n",
        "    dataset_dicts = []\n",
        "    labels_dict = create_labels_dict(image_root)  # Get category mappings\n",
        "\n",
        "    for farm_folder in os.listdir(image_root):\n",
        "        farm_path = os.path.join(image_root, farm_folder)\n",
        "        if not os.path.isdir(farm_path):\n",
        "            continue  # Skip non-folder files\n",
        "\n",
        "        for class_folder in os.listdir(farm_path):\n",
        "            class_path = os.path.join(farm_path, class_folder)\n",
        "\n",
        "            if not os.path.isdir(class_path):\n",
        "                continue  # Skip non-folder files\n",
        "\n",
        "            # Get dynamically assigned class ID\n",
        "            class_id = labels_dict.get(class_folder, None)\n",
        "            if class_id is None:\n",
        "                continue  # Ignore undefined categories\n",
        "\n",
        "            for idx, image_file in enumerate(os.listdir(class_path)):\n",
        "                image_path = os.path.join(class_path, image_file)\n",
        "                if not image_path.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                    continue  # Skip non-image files\n",
        "\n",
        "                # Verify if the file exists\n",
        "                if not os.path.exists(image_path):\n",
        "                    print(f\"Warning: Image not found - {image_path}\")\n",
        "                    continue\n",
        "\n",
        "                # Load image to get dimensions\n",
        "                img = cv2.imread(image_path)\n",
        "                if img is None:\n",
        "                    print(f\"Warning: Could not read image - {image_path}\")\n",
        "                    continue\n",
        "\n",
        "                height, width = img.shape[:2]\n",
        "\n",
        "                # Store in Detectron2 format (full image as bbox)\n",
        "                record = {\n",
        "                    \"file_name\": image_path,  # Ensure \"file_name\" is included\n",
        "                    \"image_id\": len(dataset_dicts),\n",
        "                    \"height\": height,\n",
        "                    \"width\": width,\n",
        "                    \"annotations\": [{\n",
        "                        \"bbox\": [0, 0, width, height],  # Whole image as bbox\n",
        "                        \"bbox_mode\": BoxMode.XYWH_ABS,\n",
        "                        \"category_id\": class_id,\n",
        "                    }]\n",
        "                }\n",
        "                dataset_dicts.append(record)  # ‚úÖ Append properly\n",
        "\n",
        "    return dataset_dicts, labels_dict\n"
      ],
      "metadata": {
        "id": "Y7JokZeUYeI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
        "\n",
        "# Define dataset path\n",
        "image_root = \"/content/drive/My Drive/NewBrunswick\"\n",
        "\n",
        "# Generate dataset dictionary & labels\n",
        "dataset_dicts, labels_dict = get_newbrunswick_dataset(image_root)\n",
        "\n",
        "# Register dataset\n",
        "DatasetCatalog.register(\"newbrunswick_dataset\", lambda: dataset_dicts)\n",
        "MetadataCatalog.get(\"newbrunswick_dataset\").set(\n",
        "    thing_classes=list(labels_dict.keys())  # Use dynamically created class names\n",
        ")\n",
        "\n",
        "print(\"Dataset registered successfully!\")\n",
        "print(\"Final class mapping:\", labels_dict)\n"
      ],
      "metadata": {
        "id": "sjWkrA4kYeLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultTrainer\n",
        "from detectron2 import model_zoo\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Define output directory in Google Drive\n",
        "output_dir = \"/content/drive/My Drive/detectron2_output/\"\n",
        "os.makedirs(output_dir, exist_ok=True)  # Create folder if it doesn't exist\n",
        "\n",
        "# Detectron2 Training Configuration\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.DATASETS.TRAIN = (\"newbrunswick_dataset\",)\n",
        "cfg.DATASETS.TEST = ()\n",
        "cfg.DATALOADER.NUM_WORKERS = 4\n",
        "cfg.SOLVER.IMS_PER_BATCH = 2\n",
        "cfg.SOLVER.BASE_LR = 0.001\n",
        "cfg.SOLVER.MAX_ITER = 2000\n",
        "cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = len(labels_dict)  # Use dynamically assigned classes\n",
        "\n",
        "# Set the output directory to Google Drive\n",
        "cfg.OUTPUT_DIR = output_dir\n",
        "\n",
        "# Train Model\n",
        "trainer = DefaultTrainer(cfg)\n",
        "trainer.resume_or_load(resume=False)\n",
        "trainer.train()\n",
        "\n",
        "# Move the trained model to Google Drive\n",
        "source_path = os.path.join(cfg.OUTPUT_DIR, \"model_final.pth\")\n",
        "destination_path = \"/content/drive/My Drive/model3_final.pth\"  # Change this if needed\n",
        "\n",
        "if os.path.exists(source_path):\n",
        "    shutil.copy(source_path, destination_path)\n",
        "    print(f\"Model copied to {destination_path}\")\n",
        "else:\n",
        "    print(\"Model training failed, file not found.\")\n"
      ],
      "metadata": {
        "id": "7ckfjkYDYeOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pipeline to Use Both Models"
      ],
      "metadata": {
        "id": "KAsk0xr5aYZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "import itertools\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.data import MetadataCatalog, DatasetCatalog, build_detection_test_loader\n",
        "from detectron2.structures import Boxes, Instances\n",
        "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
        "from detectron2 import model_zoo  # Corrected Import\n",
        "\n",
        "# ========================== Fix NameError: Define Configs ========================== #\n",
        "cfg1 = get_cfg()\n",
        "cfg1.merge_from_file(model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg1.MODEL.WEIGHTS = \"/content/drive/My Drive/model2_final.pth\"\n",
        "cfg1.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set confidence threshold\n",
        "cfg1.MODEL.DEVICE = \"cuda\"  # Use GPU if available\n",
        "predictor1 = DefaultPredictor(cfg1)\n",
        "\n",
        "cfg2 = get_cfg()\n",
        "cfg2.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg2.MODEL.WEIGHTS = \"/content/drive/My Drive/model3_final.pth\"\n",
        "cfg2.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Set confidence threshold\n",
        "cfg2.MODEL.DEVICE = \"cuda\"  # Use GPU if available\n",
        "predictor2 = DefaultPredictor(cfg2)\n",
        "\n",
        "# ========================== Dataset Registration ========================== #\n",
        "dataset_test = DatasetCatalog.get(\"cows_test\")\n",
        "\n",
        "# Function to filter missing files\n",
        "def filter_missing_files(dataset):\n",
        "    return [d for d in dataset if cv2.imread(d[\"file_name\"]) is not None]\n",
        "\n",
        "filtered_test = filter_missing_files(dataset_test)\n",
        "\n",
        "# Register test dataset (NO AUGMENTATION)\n",
        "def get_test_data():\n",
        "    return filtered_test\n",
        "\n",
        "if \"cows_test_balanced\" in DatasetCatalog.list():\n",
        "    DatasetCatalog.remove(\"cows_test_balanced\")\n",
        "    MetadataCatalog.remove(\"cows_test_balanced\")\n",
        "\n",
        "DatasetCatalog.register(\"cows_test_balanced\", get_test_data)\n",
        "MetadataCatalog.get(\"cows_test_balanced\").thing_classes = [\"Young Cows\", \"Dry Cows\", \"Mature Milking Cow\", \"Pregnant\"]\n",
        "\n",
        "# ========================== Custom Test Data Mapper ========================== #\n",
        "def custom_test_mapper(dataset_dict):\n",
        "    dataset_dict = dataset_dict.copy()\n",
        "    image = cv2.imread(dataset_dict[\"file_name\"])\n",
        "    if image is None:\n",
        "        return None\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    dataset_dict[\"image\"] = torch.as_tensor(image.transpose(2, 0, 1))\n",
        "\n",
        "    height, width = image.shape[:2]\n",
        "    instances = Instances((height, width))\n",
        "\n",
        "    # Convert bounding boxes correctly\n",
        "    boxes = [ann[\"bbox\"] for ann in dataset_dict[\"annotations\"]]\n",
        "    boxes = np.array(boxes, dtype=np.float32).reshape(-1, 4)  # Ensure proper shape\n",
        "    boxes = Boxes(torch.as_tensor(boxes, dtype=torch.float32))\n",
        "\n",
        "    instances.gt_boxes = boxes\n",
        "    instances.gt_classes = torch.as_tensor([ann[\"category_id\"] for ann in dataset_dict[\"annotations\"]], dtype=torch.int64)\n",
        "\n",
        "    dataset_dict[\"instances\"] = instances\n",
        "    return dataset_dict\n",
        "\n",
        "# ========================== Run Evaluation & Confusion Matrix ========================== #\n",
        "test_loader = build_detection_test_loader(cfg1, \"cows_test_balanced\", mapper=custom_test_mapper)\n",
        "categories = MetadataCatalog.get(\"cows_test_balanced\").thing_classes  # Get class names\n",
        "\n",
        "true_labels = []\n",
        "pred_labels = []\n",
        "\n",
        "for batch in test_loader:\n",
        "    file_path = batch[0][\"file_name\"]  # Get image filename\n",
        "    images = batch[0][\"image\"].cuda()  # Move images to GPU\n",
        "    gt_classes = batch[0][\"instances\"].gt_classes.cpu().numpy()  # Get ground truth labels\n",
        "    true_labels.extend(gt_classes)\n",
        "\n",
        "    # Convert tensor image to NumPy for prediction\n",
        "    img_numpy = images.cpu().numpy().transpose(1, 2, 0)\n",
        "\n",
        "    outputs = predictor1(img_numpy)  # Run model prediction\n",
        "    pred_instances = outputs[\"instances\"]\n",
        "\n",
        "    # Check if the model made any predictions\n",
        "    if pred_instances.has(\"pred_classes\") and len(pred_instances.pred_classes) > 0:\n",
        "        pred_classes = pred_instances.pred_classes.cpu().numpy()\n",
        "        scores = pred_instances.scores.cpu().numpy()  # Confidence scores\n",
        "\n",
        "        print(f\" Image: {file_path}\")\n",
        "        print(f\"   - Ground Truth: {[categories[idx] for idx in gt_classes]}\")\n",
        "        print(f\"   - Predicted Classes: {[categories[idx] for idx in pred_classes]}\")\n",
        "        print(f\"   - Confidence Scores: {scores}\")\n",
        "\n",
        "        # Select the highest-confidence prediction that is above the threshold\n",
        "        high_conf_preds = [(pred_classes[i], scores[i]) for i in range(len(scores)) if scores[i] > 0.5]\n",
        "\n",
        "        if high_conf_preds:\n",
        "            best_prediction = max(high_conf_preds, key=lambda x: x[1])[0]  # Get highest confidence class\n",
        "            pred_labels.append(best_prediction)\n",
        "        else:\n",
        "            pred_labels.append(-1)  # No high-confidence prediction\n",
        "    else:\n",
        "        print(f\"   -  No valid predictions for this image!\")\n",
        "        pred_labels.append(-1)  # No prediction case\n",
        "\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(true_labels, pred_labels)\n",
        "\n",
        "# ========================== Plot Confusion Matrix ========================== #\n",
        "def plot_confusion_matrix(cm, class_names):\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation=\"nearest\", cmap=plt.cm.Blues)\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.colorbar()\n",
        "\n",
        "    tick_marks = np.arange(len(class_names))\n",
        "    plt.xticks(tick_marks, class_names, rotation=45)\n",
        "    plt.yticks(tick_marks, class_names)\n",
        "\n",
        "    # Add text annotations\n",
        "    fmt = \"d\"\n",
        "    thresh = cm.max() / 2.0\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Display confusion matrix\n",
        "class_names = MetadataCatalog.get(\"cows_test_balanced\").thing_classes\n",
        "plot_confusion_matrix(cm, class_names)\n"
      ],
      "metadata": {
        "id": "v0eLMXIbYeQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GRAD CAM"
      ],
      "metadata": {
        "id": "Dfau33epangB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install grad-cam"
      ],
      "metadata": {
        "id": "FGWTBYL8YeTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2.utils.visualizer import Visualizer, ColorMode\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import preprocess_image as gradcam_preprocess\n",
        "from pytorch_grad_cam.utils.model_targets import FasterRCNNBoxScoreTarget\n",
        "\n",
        "# Load the trained Detectron2 model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Adjust based on your dataset\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/My Drive/model1_final.pth\"  # Change to your trained model path\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
        "\n",
        "# Initialize model manually for Grad-CAM\n",
        "model = build_model(cfg)\n",
        "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
        "model.eval()\n",
        "\n",
        "# Register Grad-CAM Hook\n",
        "feature_maps = None\n",
        "gradients = None\n",
        "\n",
        "def forward_hook(module, input, output):\n",
        "    global feature_maps\n",
        "    feature_maps = output\n",
        "\n",
        "def backward_hook(module, grad_in, grad_out):\n",
        "    global gradients\n",
        "    gradients = grad_out[0]\n",
        "\n",
        "# Register hooks to a deeper convolutional layer (res4 for better feature visualization)\n",
        "target_layer = model.backbone.bottom_up.res4\n",
        "target_layer.register_forward_hook(forward_hook)\n",
        "target_layer.register_full_backward_hook(backward_hook)\n",
        "\n",
        "# Initialize predictor\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Function to get 8 random images from test folder\n",
        "def get_random_images(folder, num_images=8):\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Check valid image formats\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    if len(image_paths) == 0:\n",
        "        raise ValueError(\"‚ùå ERROR: No images found in test folder!\")\n",
        "\n",
        "    random.shuffle(image_paths)\n",
        "    return image_paths[:num_images]\n",
        "\n",
        "# Function to generate Grad-CAM heatmap\n",
        "def generate_grad_cam(image_path):\n",
        "    global feature_maps, gradients\n",
        "\n",
        "    torch.cuda.empty_cache()  # Free up GPU memory\n",
        "\n",
        "    # Load and preprocess image\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"Error: Unable to load image at '{image_path}'. Check the path.\")\n",
        "        return None, None\n",
        "\n",
        "    img = cv2.resize(img, (512, 512))  # Resize to avoid memory overflow\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Convert to tensor and send to GPU\n",
        "    img_tensor = torch.tensor(img).permute(2, 0, 1).float().unsqueeze(0).cuda()\n",
        "\n",
        "    # Forward pass\n",
        "    model.zero_grad()\n",
        "    inputs = [{\"image\": img_tensor.squeeze()}]\n",
        "    outputs = model(inputs)[0]  # Extract first output dictionary\n",
        "\n",
        "    if \"instances\" not in outputs or len(outputs[\"instances\"]) == 0:\n",
        "        print(\"No objects detected in\", image_path)\n",
        "        return img, None\n",
        "\n",
        "    instances = outputs[\"instances\"]\n",
        "    scores = instances.scores\n",
        "    highest_class_idx = scores.argmax()\n",
        "    scores[highest_class_idx].backward()\n",
        "\n",
        "    # Compute Grad-CAM heatmap\n",
        "    pooled_gradients = torch.mean(gradients, dim=[0, 2, 3])  # Average over height & width\n",
        "    feature_maps = feature_maps.squeeze(0)  # Remove batch dimension\n",
        "    pooled_gradients = pooled_gradients.view(feature_maps.shape[0], 1, 1)\n",
        "\n",
        "    cam = torch.sum(pooled_gradients * feature_maps, dim=0).cpu().detach().numpy()\n",
        "    cam = np.maximum(cam, 0)  # Apply ReLU\n",
        "    cam = cv2.resize(cam, (img.shape[1], img.shape[0]))  # Resize to match original image size\n",
        "\n",
        "    # Overlay the heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam / cam.max()), cv2.COLORMAP_JET)\n",
        "    overlayed_img = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    return img, overlayed_img  # Return original & heatmap image\n",
        "\n",
        "# Run Grad-CAM on 8 Random Images & Display in Grid\n",
        "test_folder = \"/content/drive/My Drive/test\"  # Change this to your test directory\n",
        "selected_images = get_random_images(test_folder, num_images=8)\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 2x4 grid for displaying images\n",
        "\n",
        "for i, img_path in enumerate(selected_images):\n",
        "    row, col = divmod(i, 4)\n",
        "\n",
        "    original, heatmap = generate_grad_cam(img_path)\n",
        "    if heatmap is None:\n",
        "        continue  # Skip images with no detections\n",
        "\n",
        "    # Display results\n",
        "    axes[row, col].imshow(heatmap)\n",
        "    axes[row, col].set_title(f\"Image {i+1}\")\n",
        "    axes[row, col].axis(\"off\")\n",
        "\n",
        "# Show all Grad-CAM results\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ryUinkTTYeVs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PBE"
      ],
      "metadata": {
        "id": "_2J8XagFbARN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from detectron2.engine import DefaultPredictor\n",
        "from detectron2.config import get_cfg\n",
        "from detectron2 import model_zoo\n",
        "from detectron2.modeling import build_model\n",
        "from detectron2.checkpoint import DetectionCheckpointer\n",
        "\n",
        "# Load the trained Detectron2 model\n",
        "cfg = get_cfg()\n",
        "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\"))\n",
        "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 4  # Adjust based on your dataset\n",
        "cfg.MODEL.WEIGHTS = \"/content/drive/My Drive/model1_final.pth\"  # Change to your trained model path\n",
        "cfg.MODEL.DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"  # Use GPU if available\n",
        "\n",
        "# Initialize model\n",
        "model = build_model(cfg)\n",
        "DetectionCheckpointer(model).load(cfg.MODEL.WEIGHTS)\n",
        "model.eval()\n",
        "\n",
        "# Initialize predictor for normal inference\n",
        "predictor = DefaultPredictor(cfg)\n",
        "\n",
        "# Function to get 8 random images from test folder\n",
        "def get_random_images(folder, num_images=8):\n",
        "    image_paths = []\n",
        "    for root, _, files in os.walk(folder):\n",
        "        for file in files:\n",
        "            if file.lower().endswith(('.jpg', '.jpeg', '.png')):  # Check valid image formats\n",
        "                image_paths.append(os.path.join(root, file))\n",
        "\n",
        "    if len(image_paths) == 0:\n",
        "        raise ValueError(\"ERROR: No images found in test folder!\")\n",
        "\n",
        "    random.shuffle(image_paths)\n",
        "    return image_paths[:num_images]\n",
        "\n",
        "# Occlusion Sensitivity Function\n",
        "def perturb_image(image, mask_size=50, stride=25):\n",
        "    \"\"\"\n",
        "    Iteratively occludes different parts of the image and measures impact on predictions.\n",
        "    Generates an importance heatmap showing regions most sensitive to occlusion.\n",
        "    \"\"\"\n",
        "    h, w, _ = image.shape\n",
        "    heatmap = np.zeros((h, w))\n",
        "\n",
        "    # Run original prediction\n",
        "    original_output = predictor(image)\n",
        "    original_score = original_output[\"instances\"].scores.max().item() if len(original_output[\"instances\"]) > 0 else 0\n",
        "\n",
        "    # Slide occlusion window across image\n",
        "    for y in range(0, h, stride):\n",
        "        for x in range(0, w, stride):\n",
        "            # Create occlusion mask\n",
        "            occluded_image = image.copy()\n",
        "            occluded_image[y:y+mask_size, x:x+mask_size] = 0  # Black-out occluded area\n",
        "\n",
        "            # Run prediction on occluded image\n",
        "            occluded_output = predictor(occluded_image)\n",
        "            occluded_score = occluded_output[\"instances\"].scores.max().item() if len(occluded_output[\"instances\"]) > 0 else 0\n",
        "\n",
        "            # Compute score drop (higher = more important)\n",
        "            heatmap[y:y+mask_size, x:x+mask_size] = original_score - occluded_score\n",
        "\n",
        "    # Normalize heatmap\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "    return heatmap\n",
        "\n",
        "# Run Perturbation-Based Explainability on 8 Random Images & Display in Grid\n",
        "test_folder = \"/content/drive/My Drive/test\"  # Change this to your test directory\n",
        "selected_images = get_random_images(test_folder, num_images=8)\n",
        "\n",
        "fig, axes = plt.subplots(2, 4, figsize=(20, 10))  # 2x4 grid for displaying images\n",
        "\n",
        "for i, img_path in enumerate(selected_images):\n",
        "    row, col = divmod(i, 4)\n",
        "\n",
        "    # Load image\n",
        "    img = cv2.imread(img_path)\n",
        "    if img is None:\n",
        "        print(f\"ERROR: Could not load {img_path}\")\n",
        "        continue\n",
        "    img = cv2.resize(img, (512, 512))  # Resize for consistency\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
        "\n",
        "    # Compute occlusion sensitivity heatmap\n",
        "    heatmap = perturb_image(img)\n",
        "\n",
        "    # Convert to color heatmap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
        "    overlayed_img = cv2.addWeighted(img, 0.5, heatmap, 0.5, 0)\n",
        "\n",
        "    # Display results\n",
        "    axes[row, col].imshow(overlayed_img)\n",
        "    axes[row, col].set_title(f\"Image {i+1}\")\n",
        "    axes[row, col].axis(\"off\")\n",
        "\n",
        "# Show all perturbation-based results\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "65jMyYaWYeYF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}